{"cells":[{"cell_type":"markdown","source":["<a href=\"https://vigneashpandiyan.github.io/publications/Codes/\" target=\"_blank\" rel=\"noopener noreferrer\">\n","  <img src=\"https://vigneashpandiyan.github.io/images/Link.png\"\n","       style=\"max-width: 800px; width: 100%; height: auto;\">\n","</a>"],"metadata":{"id":"biw_Ue5FsKi5"},"id":"biw_Ue5FsKi5"},{"cell_type":"markdown","id":"reverse-interview","metadata":{"id":"reverse-interview"},"source":["# Vector Operations: Scalar Multiplication, Sum and Dot Product of Vectors"]},{"cell_type":"markdown","id":"parental-conclusion","metadata":{"id":"parental-conclusion"},"source":["In this module you will use Python and `NumPy` functions to perform main vector operations: scalar multiplication, sum of vectors and their dot product. You will also investigate the speed of calculations using loop and vectorized forms of these main linear algebra operations"]},{"cell_type":"markdown","id":"advance-butler","metadata":{"id":"advance-butler"},"source":["## Packages\n","\n","Load the `NumPy` package to access its functions."]},{"cell_type":"code","execution_count":null,"id":"promotional-buffer","metadata":{"id":"promotional-buffer"},"outputs":[],"source":["import numpy as np"]},{"cell_type":"markdown","id":"severe-studio","metadata":{"id":"severe-studio"},"source":["<a name='1'></a>\n","## 1 - Scalar Multiplication and Sum of Vectors"]},{"cell_type":"markdown","id":"ethical-success","metadata":{"id":"ethical-success"},"source":["<a name='1.1'></a>\n","### 1.1 - Visualization of a Vector $v\\in\\mathbb{R}^2$\n","\n","You already have seen in the videos and labs, that vectors can be visualized as arrows, and it is easy to do it for a $v\\in\\mathbb{R}^2$, e.g.\n","$v=\\begin{bmatrix}\n","          1 & 3\n","\\end{bmatrix}^T$\n","\n","The following code will show the visualization."]},{"cell_type":"code","execution_count":null,"id":"korean-landing","metadata":{"id":"korean-landing"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","import numpy as np\n","\n","def plot_vectors(list_v, list_label, list_color):\n","    _, ax = plt.subplots(figsize=(10, 10))\n","    ax.tick_params(axis='x', labelsize=14)\n","    ax.tick_params(axis='y', labelsize=14)\n","    ax.set_xticks(np.arange(-10, 10))\n","    ax.set_yticks(np.arange(-10, 10))\n","\n","\n","    plt.axis([-10, 10, -10, 10])\n","    for i, v in enumerate(list_v):\n","        sgn = 0.4 * np.array([[1] if i==0 else [i] for i in np.sign(v)])\n","        plt.quiver(v[0], v[1], color=list_color[i], angles='xy', scale_units='xy', scale=1)\n","        ax.text(v[0]-0.2+sgn[0], v[1]-0.2+sgn[1], list_label[i], fontsize=14, color=list_color[i])\n","\n","    plt.grid()\n","    plt.gca().set_aspect(\"equal\")\n","    plt.show()\n","\n","v = np.array([[1],[3]])\n","# Arguments: list of vectors as NumPy arrays, labels, colors.\n","plot_vectors([v], [f\"$v$\"], [\"black\"])"]},{"cell_type":"markdown","id":"original-translator","metadata":{"id":"original-translator"},"source":["The vector is defined by its **norm (length, magnitude)** and **direction**, not its actual position. But for clarity and convenience vectors are often plotted starting in the origin (in $\\mathbb{R}^2$ it is a point $(0,0)$) ."]},{"cell_type":"markdown","id":"speaking-surgeon","metadata":{"id":"speaking-surgeon"},"source":["<a name='1.2'></a>\n","### 1.2 - Scalar Multiplication\n","\n","**Scalar multiplication** of a vector $v=\\begin{bmatrix}\n","          v_1 & v_2 & \\ldots & v_n\n","\\end{bmatrix}^T\\in\\mathbb{R}^n$ by a scalar $k$ is a vector $kv=\\begin{bmatrix}\n","          kv_1 & kv_2 & \\ldots & kv_n\n","\\end{bmatrix}^T$ (element by element multiplication). If $k>0$, then $kv$ is a vector pointing in the same direction as $v$ and it is $k$ times as long as $v$. If $k=0$, then $kv$ is a zero vector. If $k<0$, vector $kv$ will be pointing in the opposite direction. In Python you can perform this operation with a `*` operator. Check out the example below:"]},{"cell_type":"code","execution_count":null,"id":"acute-investment","metadata":{"id":"acute-investment"},"outputs":[],"source":["plot_vectors([v, 2*v, -2*v], [f\"$v$\", f\"$2v$\", f\"$-2v$\"], [\"black\", \"green\", \"blue\"])"]},{"cell_type":"markdown","id":"civil-county","metadata":{"id":"civil-county"},"source":["<a name='1.3'></a>\n","### 1.3 - Sum of Vectors\n","\n","**Sum of vectors (vector addition)** can be performed by adding the corresponding components of the vectors: if $v=\\begin{bmatrix}\n","          v_1 & v_2 & \\ldots & v_n\n","\\end{bmatrix}^T\\in\\mathbb{R}^n$ and  \n","$w=\\begin{bmatrix}\n","          w_1 & w_2 & \\ldots & w_n\n","\\end{bmatrix}^T\\in\\mathbb{R}^n$, then $v + w=\\begin{bmatrix}\n","          v_1 + w_1 & v_2 + w_2 & \\ldots & v_n + w_n\n","\\end{bmatrix}^T\\in\\mathbb{R}^n$. The so-called **parallelogram law** gives the rule for vector addition. For two vectors $u$ and $v$ represented by the adjacent sides (both in magnitude and direction) of a parallelogram drawn from a point, the vector sum $u+v$ is is represented by the diagonal of the parallelogram drawn from the same point.\n","\n","In Python you can either use `+` operator or `NumPy` function `np.add()`. In the following code you can uncomment the line to check that the result will be the same:"]},{"cell_type":"code","execution_count":null,"id":"acoustic-heath","metadata":{"id":"acoustic-heath"},"outputs":[],"source":["v = np.array([[1],[3]])\n","w = np.array([[4],[-1]])\n","\n","plot_vectors([v, w, v + w], [f\"$v$\", f\"$w$\", f\"$v + w$\"], [\"black\", \"black\", \"red\"])\n","# plot_vectors([v, w, np.add(v, w)], [f\"$v$\", f\"$w$\", f\"$v + w$\"], [\"black\", \"black\", \"red\"])"]},{"cell_type":"markdown","id":"nearby-portal","metadata":{"id":"nearby-portal"},"source":["<a name='1.4'></a>\n","### 1.4 - Norm of a Vector\n","\n","The norm of a vector $v$ is denoted as $\\lvert v\\rvert$. It is a nonnegative number that describes the extent of the vector in space (its length). The norm of a vector can be found using `NumPy` function `np.linalg.norm()`:"]},{"cell_type":"code","execution_count":null,"id":"spare-timing","metadata":{"id":"spare-timing"},"outputs":[],"source":["print(\"Norm of a vector v is\", np.linalg.norm(v))"]},{"cell_type":"markdown","id":"talented-survey","metadata":{"id":"talented-survey"},"source":["<a name='2'></a>\n","## 2 - Dot Product"]},{"cell_type":"markdown","id":"sunset-transmission","metadata":{"id":"sunset-transmission"},"source":["<a name='2.1'></a>\n","### 2.1 - Algebraic Definition of the Dot Product\n","\n","The **dot product** (or **scalar product**) is an algebraic operation that takes two vectors $x=\\begin{bmatrix}\n","          x_1 & x_2 & \\ldots & x_n\n","\\end{bmatrix}^T\\in\\mathbb{R}^n$ and  \n","$y=\\begin{bmatrix}\n","          y_1 & y_2 & \\ldots & y_n\n","\\end{bmatrix}^T\\in\\mathbb{R}^n$ and returns a single scalar. The dot product can be represented with a dot operator $x\\cdot y$ and defined as:\n","\n","$$x\\cdot y = \\sum_{i=1}^{n} x_iy_i = x_1y_1+x_2y_2+\\ldots+x_ny_n \\tag{1}$$"]},{"cell_type":"markdown","id":"meaningful-timer","metadata":{"id":"meaningful-timer"},"source":["<a name='2.2'></a>\n","### 2.2 - Dot Product using Python\n","\n","The simplest way to calculate dot product in Python is to take the sum of element by element multiplications. You can define the vectors $x$ and $y$ by listing their coordinates:"]},{"cell_type":"code","execution_count":null,"id":"musical-battlefield","metadata":{"id":"musical-battlefield"},"outputs":[],"source":["x = [1, -2, -5]\n","y = [4, 3, -1]"]},{"cell_type":"markdown","id":"plastic-temple","metadata":{"id":"plastic-temple"},"source":["Next, let’s define a function `dot(x,y)` for the dot product calculation:"]},{"cell_type":"code","execution_count":null,"id":"signed-syndicate","metadata":{"id":"signed-syndicate"},"outputs":[],"source":["def dot(x, y):\n","    s=0\n","    for xi, yi in zip(x, y):\n","        s += xi * yi\n","    return s"]},{"cell_type":"markdown","id":"upper-highlight","metadata":{"id":"upper-highlight"},"source":["For the sake of simplicity, let’s assume that the vectors passed to the above function are always of the same size, so that you don’t need to perform additional checks.\n","\n","Now everything is ready to perform the dot product calculation calling the function `dot(x,y)`:"]},{"cell_type":"code","execution_count":null,"id":"amazing-broadway","metadata":{"id":"amazing-broadway"},"outputs":[],"source":["print(\"The dot product of x and y is\", dot(x, y))"]},{"cell_type":"markdown","id":"banned-dallas","metadata":{"id":"banned-dallas"},"source":["Dot product is very a commonly used operator, so `NumPy` linear algebra package provides quick way to calculate it using function `np.dot()`:"]},{"cell_type":"code","execution_count":null,"id":"accessible-kinase","metadata":{"id":"accessible-kinase"},"outputs":[],"source":["print(\"np.dot(x,y) function returns dot product of x and y:\", np.dot(x, y))"]},{"cell_type":"markdown","id":"friendly-beast","metadata":{"id":"friendly-beast"},"source":["Note that you did not have to define vectors $x$ and $y$ as `NumPy` arrays, the function worked even with the lists. But there are alternative functions in Python, such as explicit operator `@` for the dot product, which can be applied only to the `NumPy` arrays. You can run the following cell to check that."]},{"cell_type":"code","execution_count":null,"id":"built-paper","metadata":{"id":"built-paper"},"outputs":[],"source":["print(\"This line output is a dot product of x and y: \", np.array(x) @ np.array(y))\n","\n","print(\"\\nThis line output is an error:\")\n","try:\n","    print(x @ y)\n","except TypeError as err:\n","    print(err)"]},{"cell_type":"markdown","id":"central-museum","metadata":{"id":"central-museum"},"source":["As both `np.dot()` and `@` operators are commonly used, it is recommended to define vectors as `NumPy` arrays to avoid errors. Let's redefine vectors $x$ and $y$ as `NumPy` arrays to be safe:"]},{"cell_type":"code","execution_count":null,"id":"israeli-jumping","metadata":{"id":"israeli-jumping"},"outputs":[],"source":["x = np.array(x)\n","y = np.array(y)"]},{"cell_type":"markdown","id":"wicked-queensland","metadata":{"id":"wicked-queensland"},"source":["<a name='2.3'></a>\n","### 2.3 - Speed of Calculations in Vectorized Form\n","\n","Dot product operations in Machine Learning applications are applied to the large vectors with hundreds or thousands of coordinates (called **high dimensional vectors**). Training models based on large datasets often takes hours and days even on powerful machines. Speed of calculations is crucial for the training and deployment of your models.\n","\n","It is important to understand the difference in the speed of calculations using vectorized and the loop forms of the vectors and functions. In the loop form operations are performed one by one, while in the vectorized form they can be performed in parallel. In the section above you defined loop version of the dot product calculation (function `dot()`), while `np.dot()` and `@` are the functions representing vectorized form.\n","\n","Let's perform a simple experiment to compare their speed. Define new vectors $a$ and $b$ of the same size $1,000,000$:"]},{"cell_type":"code","execution_count":null,"id":"amino-creation","metadata":{"id":"amino-creation"},"outputs":[],"source":["a = np.random.rand(1000000)\n","b = np.random.rand(1000000)"]},{"cell_type":"markdown","id":"facial-refrigerator","metadata":{"id":"facial-refrigerator"},"source":["Use `time.time()` function to evaluate amount of time (in seconds) required to calculate dot product using the function `dot(x,y)` which you defined above:"]},{"cell_type":"code","execution_count":null,"id":"handed-influence","metadata":{"id":"handed-influence"},"outputs":[],"source":["import time\n","\n","tic = time.time()\n","c = dot(a,b)\n","toc = time.time()\n","print(\"Dot product: \", c)\n","print (\"Time for the loop version:\" + str(1000*(toc-tic)) + \" ms\")"]},{"cell_type":"markdown","id":"accessible-sherman","metadata":{"id":"accessible-sherman"},"source":["Now compare it with the speed of the vectorized versions:"]},{"cell_type":"code","execution_count":null,"id":"determined-cooking","metadata":{"id":"determined-cooking"},"outputs":[],"source":["tic = time.time()\n","c = np.dot(a,b)\n","toc = time.time()\n","print(\"Dot product: \", c)\n","print (\"Time for the vectorized version, np.dot() function: \" + str(1000*(toc-tic)) + \" ms\")"]},{"cell_type":"code","execution_count":null,"id":"scientific-empty","metadata":{"id":"scientific-empty"},"outputs":[],"source":["tic = time.time()\n","c = a @ b\n","toc = time.time()\n","print(\"Dot product: \", c)\n","print (\"Time for the vectorized version, @ function: \" + str(1000*(toc-tic)) + \" ms\")"]},{"cell_type":"markdown","id":"useful-sleeping","metadata":{"id":"useful-sleeping"},"source":["You can see that vectorization is extremely beneficial in terms of the speed of calculations!"]},{"cell_type":"markdown","id":"postal-latin","metadata":{"id":"postal-latin"},"source":["<a name='2.4'></a>\n","### 2.4 - Geometric Definition of the Dot Product\n","\n","In [Euclidean space](https://en.wikipedia.org/wiki/Euclidean_space), a Euclidean vector has both magnitude and direction. The dot product of two vectors $x$ and $y$ is defined by:\n","\n","$$x\\cdot y = \\lvert x\\rvert \\lvert y\\rvert \\cos(\\theta),\\tag{2}$$\n","\n","where $\\theta$ is the angle between the two vectors.\n","\n","This provides an easy way to test the orthogonality between vectors. If $x$ and $y$ are orthogonal (the angle between vectors is $90^{\\circ}$), then since $\\cos(90^{\\circ})=0$, it implies that **the dot product of any two orthogonal vectors must be $0$**. Let's test it, taking two vectors $i$ and $j$ we know are orthogonal:"]},{"cell_type":"code","execution_count":null,"id":"shared-climb","metadata":{"id":"shared-climb"},"outputs":[],"source":["i = np.array([1, 0, 0])\n","j = np.array([0, 1, 0])\n","print(\"The dot product of i and j is\", dot(i, j))"]},{"cell_type":"markdown","metadata":{"id":"thermal-railway"},"source":["<a name='2.5'></a>\n","### 2.5 - Application of the Dot Product: Vector Similarity\n","\n","Geometric definition of a dot product is used in one of the applications - to evaluate **vector similarity**. In Natural Language Processing (NLP) words or phrases from vocabulary are mapped to a corresponding vector of real numbers. Similarity between two vectors can be defined as a cosine of the angle between them. When they point in the same direction, their similarity is 1 and it decreases with the increase of the angle.\n","\n","Then equation $(2)$ can be rearranged to evaluate cosine of the angle between vectors:\n","\n","$\\cos(\\theta)=\\frac{x \\cdot y}{\\lvert x\\rvert \\lvert y\\rvert}.\\tag{3}$\n","\n","Zero value corresponds to the zero similarity between vectors (and words corresponding to those vectors). Largest value is when vectors point in the same direction, lowest value is when vectors point in the opposite directions.\n","\n","This example of vector similarity is given to link the material with the Machine Learning applications. There will be no actual implementation of it in this Course. Some examples of implementation can be found in the Natual Language Processing Specialization.\n","\n","Well done, you have finished this lab!"],"id":"thermal-railway"},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","def plot_vectors(list_v, list_label, list_color):\n","    _, ax = plt.subplots(figsize=(8, 8))\n","    ax.tick_params(axis='x', labelsize=14)\n","    ax.tick_params(axis='y', labelsize=14)\n","\n","    # Setting the grid limits\n","    limit = 10\n","    ax.set_xticks(np.arange(-limit, limit+1))\n","    ax.set_yticks(np.arange(-limit, limit+1))\n","    plt.axis([-limit, limit, -limit, limit])\n","\n","    for i, v in enumerate(list_v):\n","        # Using quiver to plot vectors from the origin (0,0)\n","        plt.quiver(0, 0, v[0], v[1], color=list_color[i], angles='xy', scale_units='xy', scale=1)\n","\n","        # Label placement logic\n","        ax.text(v[0], v[1], f\" {list_label[i]}\", fontsize=14, color=list_color[i], fontweight='bold')\n","\n","    plt.grid(True, linestyle='--', alpha=0.6)\n","    plt.axhline(0, color='black', linewidth=1)\n","    plt.axvline(0, color='black', linewidth=1)\n","    plt.gca().set_aspect(\"equal\")\n","    plt.title(\"Vector Similarity in 2D Space\\n(Smaller Angle = Higher Dot Product)\", fontsize=16)\n","    plt.show()\n","\n","# --- Use Case: Image Recognition ---\n","# Component X: Organic Shapes / Texture\n","# Component Y: Metallic / Geometric Edges\n","\n","v_target = np.array([8, 2])    # The Reference (e.g., a Golden Retriever)\n","v_similar = np.array([7, 3])   # Similar (e.g., a Labrador) - Small angle\n","v_unrelated = np.array([1, 8]) # Unrelated (e.g., a Building) - Wide angle\n","\n","vectors = [v_target, v_similar, v_unrelated]\n","labels = ['Target (Dog)', 'Similar (Dog)', 'Unrelated (Building)']\n","colors = ['#1f77b4', '#2ca02c', '#d62728']\n","\n","plot_vectors(vectors, labels, colors)"],"metadata":{"id":"mrfLJwoGHn3A"},"id":"mrfLJwoGHn3A","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["\n","\n","> Why this plot explains Similarity:\n","\n","\n","\n","1.   The Angle ($\\theta$): Notice how the blue and green arrows are \"hugging\" each other. Because they point in nearly the same direction, their dot product is very large. In Image Recognition, this means the visual patterns (pixels/edges) match closely.\n","2.   Orthogonality: The red arrow (Building) is pointing almost vertically, while the blue arrow is mostly horizontal. Their dot product will be very low because they do not share the same \"feature space\".\n","\n","\n","\n","*   The Math Connection:\n","\n","---\n","\n","\n","High Similarity: $\\cos(0^\\circ) = 1$\n","\n","\n","---\n","\n","\n","No Similarity: $\\cos(90^\\circ) = 0$\n","\n","---\n","\n","\n","Opposite: $\\cos(180^\\circ) = -1$\n","\n","---\n","\n"],"metadata":{"id":"3flq4osODvNZ"},"id":"3flq4osODvNZ"},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# 1. Define our Feature Matrix (Rows = Images, Columns = Features)\n","# Features: [Organic/Fur, Metallic/Geometry]\n","features = np.array([\n","    [8, 2],  # Image 0: Golden Retriever\n","    [7, 3],  # Image 1: Labrador\n","    [2, 9],  # Image 2: Skyscrapers\n","    [1, 8],  # Image 3: Sports Car\n","    [7, 2]   # Image 4: Poodle\n","])\n","\n","labels = ['Retriever', 'Labrador', 'Skyscrapers', 'Sports Car', 'Poodle']\n","\n","# 2. Normalize the rows to unit length (so dot product = cosine similarity)\n","norms = np.linalg.norm(features, axis=1, keepdims=True)\n","normalized_features = features / norms\n","\n","# 3. Compute the Similarity Matrix using the Dot Product\n","# Matrix Multiplication: (5x2) @ (2x5) = (5x5) matrix of all pairs\n","similarity_matrix = np.dot(normalized_features, normalized_features.T)\n","\n","# 4. Visualization\n","plt.figure(figsize=(10, 8))\n","sns.heatmap(similarity_matrix, annot=True, cmap='YlGnBu',\n","            xticklabels=labels, yticklabels=labels)\n","\n","plt.title(\"2D Similarity Matrix (Dot Product)\", fontsize=16)\n","plt.show()"],"metadata":{"id":"OJrnThqbMunn"},"id":"OJrnThqbMunn","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","# --- 1. The Database Matrix (2D) ---\n","# Each row represents an image in our database.\n","# Dimensions: [Texture/Fur, Geometry/Edges]\n","database_matrix = np.array([\n","    [9, 2],  # Index 0: Husky\n","    [8, 1],  # Index 1: Beagle\n","    [2, 8],  # Index 2: Eiffel Tower\n","    [1, 9],  # Index 3: Bridge\n","    [5, 5]   # Index 4: Abstract Art\n","])\n","db_labels = ['Husky', 'Beagle', 'Eiffel Tower', 'Bridge', 'Abstract Art']\n","\n","# --- 2. The Query Vector (1D) ---\n","# Someone uploads a photo of a 'Golden Retriever'\n","query_vector = np.array([8.5, 1.5])\n","\n","# --- 3. Vector Similarity Calculation ---\n","# Step A: Normalize for Cosine Similarity\n","db_norm = database_matrix / np.linalg.norm(database_matrix, axis=1, keepdims=True)\n","query_norm = query_vector / np.linalg.norm(query_vector)\n","\n","# Step B: The Dot Product (The Engine)\n","# This calculates similarity for ALL database items at once\n","similarities = np.dot(db_norm, query_norm)\n","\n","# --- 4. Visualization ---\n","plt.figure(figsize=(10, 5))\n","colors = ['green' if s > 0.8 else 'gray' for s in similarities]\n","plt.bar(db_labels, similarities, color=colors)\n","plt.axhline(y=0.9, color='r', linestyle='--', label='Match Threshold')\n","\n","plt.title('Vector Similarity: Query vs. Database Matrix', fontsize=14)\n","plt.ylabel('Similarity Score (Dot Product)')\n","plt.ylim(0, 1.1)\n","plt.legend()\n","\n","for i, score in enumerate(similarities):\n","    plt.text(i, score + 0.02, f\"{score:.2f}\", ha='center', fontweight='bold')\n","\n","plt.show()"],"metadata":{"id":"tE6C-Jk7NB3M"},"id":"tE6C-Jk7NB3M","execution_count":null,"outputs":[]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","\n","def cosine_similarity(v1, v2):\n","    \"\"\"Calculates similarity using the dot product formula.\"\"\"\n","    # The Dot Product measures the combined strength of matching dimensions\n","    dot_product = np.dot(v1, v2)\n","    norm_v1 = np.linalg.norm(v1)\n","    norm_v2 = np.linalg.norm(v2)\n","    return dot_product / (norm_v1 * norm_v2)\n","\n","# --- Scenario 1: Natural Language Processing (NLP) ---\n","# Each vector has 3 dimensions representing specific topics:\n","# [Technology, Nature, Sports]\n","# A value of 0.9 means the text is 90% related to that specific topic.\n","\n","# Query: \"Latest smartphone technology\"\n","# Logic: High in Tech (0.9), low in Nature (0.1), zero in Sports (0.0)\n","query_nlp = np.array([0.9, 0.1, 0.0])\n","\n","docs_nlp = {\n","    # Aligns with Tech dimension\n","    'Tech Article': np.array([0.85, 0.2, 0.05]),\n","    # Aligns with Nature dimension (orthogonal to Tech)\n","    'Nature Blog':  np.array([0.1, 0.9, 0.0]),\n","    # Aligns with Sports dimension\n","    'Sports News':  np.array([0.05, 0.05, 0.9])\n","}\n","\n","sim_nlp = [cosine_similarity(query_nlp, v) for v in docs_nlp.values()]\n","labels_nlp = list(docs_nlp.keys())\n","\n","# --- Scenario 2: Recommendation Systems (RecSys) ---\n","# Each vector has 3 dimensions representing movie genres:\n","# [Action/Sci-Fi, Romance, Documentary]\n","\n","# User Profile: What the user likes\n","# Logic: User loves Action (0.8), Sci-Fi (0.7), but dislikes Romance (0.1)\n","user_pref = np.array([0.8, 0.7, 0.1])\n","\n","items_rec = {\n","    # High in Action/Sci-Fi (Matching the user)\n","    'Action Movie':  np.array([0.9, 0.8, 0.2]),\n","    # High in Romance (Mismatching the user)\n","    'Romance Film':  np.array([0.1, 0.2, 0.9]),\n","    # High in Documentary (Neutral/Low match)\n","    'Nature Docu':   np.array([0.3, 0.3, 0.8])\n","}\n","\n","sim_rec = [cosine_similarity(user_pref, v) for v in items_rec.values()]\n","labels_rec = list(items_rec.keys())\n","\n","# --- Visualization Logic ---\n","fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6), sharey=True)\n","\n","# NLP Plot\n","ax1.bar(labels_nlp, sim_nlp, color=['#1f77b4', '#aec7e8', '#ff7f0e'])\n","ax1.set_title('Use Case 1: NLP Similarity\\n[Tech, Nature, Sports]', fontsize=14)\n","ax1.set_ylabel('Cosine Similarity Score', fontsize=12)\n","ax1.set_ylim(0, 1.1)\n","\n","# RecSys Plot\n","ax2.bar(labels_rec, sim_rec, color=['#2ca02c', '#98df8a', '#d62728'])\n","ax2.set_title('Use Case 2: Recommendation Matching\\n[Action, Romance, Documentary]', fontsize=14)\n","\n","for ax, sims in zip([ax1, ax2], [sim_nlp, sim_rec]):\n","    for i, v in enumerate(sims):\n","        ax.text(i, v + 0.02, f\"{v:.2f}\", ha='center', fontweight='bold')\n","\n","plt.tight_layout()\n","plt.show()"],"metadata":{"id":"cRSMnBF5DwYm"},"id":"cRSMnBF5DwYm","execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Breakdown of the Component Logic\n","In vector similarity, these components are often called Features or Dimensions. Here is how they function in the code:\n","\n","\n","\n","\n","\n","1.   NLP: Semantic Dimensions\n","\n","*  Dimension 1 (Tech): Measures the presence of keywords like \"processor\",\"silicon,\" or \"software.\n","* \"Dimension 2 (Nature): Measures keywords like \"forest,\" \"climate,\" or \"wildlife.\n","* \"Dimension 3 (Sports): Measures keywords like \"goal,\" \"stadium,\" or \"athlete.\n","\n","\n","\"Why it works: When you multiply the Query [0.9, 0.1, 0.0] by the Tech Article [0.85, 0.2, 0.05], the high values in the first index multiply together ($0.9 \\times 0.85 = 0.765$), leading to a high dot product.\n","\n","2.   RecSys: Preference Dimensions\n","\n","\n","*   Dimension 1 (Action/Sci-Fi): Represents the intensity or \"weight\" of explosions, futuristic technology, or fast pacing.\n","*   Dimension 2 (Romance): Represents emotional focus, relationship dynamics, or \"meet-cutes.\n","*   \"Dimension 3 (Documentary): Represents educational value, real-world footage, and factual narration.\n","\n","\n","Why it works: The dot product acts as a weighted sum. It calculates how much of the \"Action\" the user wants times how much \"Action\" the movie actually has. If both are high, the recommendation score peaks."],"metadata":{"id":"65XUhav9GCC9"},"id":"65XUhav9GCC9"}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.8.8"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":5}