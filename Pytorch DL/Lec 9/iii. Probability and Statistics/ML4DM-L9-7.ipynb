{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://vigneashpandiyan.github.io/publications/Codes/\" target=\"_blank\" rel=\"noopener noreferrer\">\n","  <img src=\"https://vigneashpandiyan.github.io/images/Link.png\"\n","       style=\"max-width: 800px; width: 100%; height: auto;\">\n","</a>"]},{"cell_type":"markdown","metadata":{"id":"aTOLgsbN69-P"},"source":["# Intro to Statistics"]},{"cell_type":"markdown","metadata":{"id":"Z68nQ0ekCYhF"},"source":["\n","\n","\n","* Review of Relevant Probability Theory\n","* *z*-scores and Outliers\n","* *p*-values\n","* Comparing Means with t-tests\n","* Confidence Intervals\n","* ANOVA: Analysis of Variance\n","* Pearson Correlation Coefficient\n","* R-Squared Coefficient of Determination\n","* Correlation vs Causation\n","* Correcting for Multiple Comparisons\n"]},{"cell_type":"markdown","metadata":{"id":"tctnFC_RJy94"},"source":["##  1: Frequentist Statistics"]},{"cell_type":"code","metadata":{"id":"Aq53QtfZJy94"},"source":["import numpy as np\n","import scipy.stats as st\n","import matplotlib.pyplot as plt\n","import seaborn as sns"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZTBb1QFuJy94"},"source":["np.random.seed(42)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mWFzBgwKJy95"},"source":["### Measures of Central Tendency"]},{"cell_type":"markdown","metadata":{"id":"2wAeml4zJy95"},"source":["Measures of central tendency provide a summary statistic on the center of a given distribution, a.k.a., the \"average\" value of the distribution."]},{"cell_type":"code","metadata":{"id":"4GYvSKu9Jy95"},"source":["x = st.skewnorm.rvs(10, size=1000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NfkkNUXcJy95"},"source":["x[0:20]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R9rXZm5pJy96"},"source":["fig, ax = plt.subplots()\n","_ = plt.hist(x, color = 'lightgray')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z8wAQUQaJy97"},"source":["#### Mean"]},{"cell_type":"markdown","metadata":{"id":"JVzPrg7AJy97"},"source":["The most common measure of central tendency, synonomous with the term \"average\", is the **mean**, often symbolized with $\\mu$ (population) or $\\bar{x}$ (sample):"]},{"cell_type":"markdown","metadata":{"id":"UoVZlrXkJy97"},"source":["$$ \\bar{x} = \\frac{\\sum_{i=1}^n x_i}{n} $$"]},{"cell_type":"code","metadata":{"id":"lnHaPyx4Jy97"},"source":["xbar = x.mean()\n","xbar"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4lSu__LtJy97"},"source":["fig, ax = plt.subplots()\n","plt.axvline(x = x.mean(), color='orange')\n","_ = plt.hist(x, color = 'lightgray')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MN9OBSxfJy98"},"source":["#### Median"]},{"cell_type":"markdown","metadata":{"id":"EwGeNSC5Jy98"},"source":["The second most common measure of central tendency is the **median**, the midpoint value in the distribution:"]},{"cell_type":"code","metadata":{"id":"uS_asMcbJy98"},"source":["np.median(x)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hr3dHF0NJy98"},"source":["The **mode** is least impacted by skew, but is typically only applicable to discrete distributions. For continuous distributions with skew (e.g., salary data), median is typically the choice measure of central tendency:"]},{"cell_type":"code","metadata":{"id":"OzX0e9HLJy98"},"source":["fig, ax = plt.subplots()\n","plt.axvline(x = np.mean(x), color='orange')\n","plt.axvline(x = np.median(x), color='green')\n","_ = plt.hist(x, color = 'lightgray')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1m-KJ5kPJy99"},"source":["### Measures of Dispersion"]},{"cell_type":"markdown","metadata":{"id":"PKJQU3eNJy99"},"source":["#### Variance"]},{"cell_type":"markdown","metadata":{"id":"ZtkXiVFVJy99"},"source":["$$ \\sigma^2 = \\frac{\\sum_{i=1}^n (x_i-\\bar{x})^2}{n} $$"]},{"cell_type":"code","metadata":{"id":"FlC38ygXJy99"},"source":["x.var()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"g07LUlduJy99"},"source":["#### Standard Deviation"]},{"cell_type":"markdown","metadata":{"id":"I1VTosKFJy99"},"source":["A straightforward derivative of variance is **standard deviation** (denoted with $\\sigma$), which is convenient because its units are on the same scale as the values in the distribution:\n","$$ \\sigma = \\sqrt{\\sigma^2} $$"]},{"cell_type":"code","metadata":{"id":"stPfptc7Jy9-"},"source":["x.var()**(1/2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JLCQyWcjJy9-"},"source":["sigma = x.std()\n","sigma"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PO_q9fL7Jy9-"},"source":["fig, ax = plt.subplots()\n","plt.axvline(x = xbar, color='orange')\n","plt.axvline(x = xbar+sigma, color='olivedrab')\n","plt.axvline(x = xbar-sigma, color='olivedrab')\n","_ = plt.hist(x, color = 'lightgray')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MtQ3n34PJy9-"},"source":["#### Standard Error"]},{"cell_type":"markdown","metadata":{"id":"waS0PbnYJy9-"},"source":["A further derivation of standard deviation is **standard error**, which is denoted with $\\sigma_\\bar{x}$:\n","$$ \\sigma_\\bar{x} = \\frac{\\sigma}{\\sqrt{n}} $$"]},{"cell_type":"code","metadata":{"id":"W9f9__wlJy9_"},"source":["sigma/(x.size)**(1/2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"toH48k6TJy9_"},"source":["st.sem(x) # defaults to 1 degree of freedom, which can be ignored with the larger data sets of ML"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OeYF2Zh2Jy9_"},"source":["st.sem(x, ddof=0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"18G3AEvTJy9_"},"source":["Standard error enables us to compare whether the means of two distributions differ *significantly*, a focus of *Intro to Stats*."]},{"cell_type":"markdown","metadata":{"id":"W8d9xMvcJy9_"},"source":["### Gaussian Distribution"]},{"cell_type":"markdown","metadata":{"id":"LG3mP9v-Jy9_"},"source":["After Carl Friedrich Gauss. Also known as **normal distribution**:"]},{"cell_type":"code","metadata":{"id":"3SsBbCQ1Jy-A"},"source":["x = np.random.normal(size=10000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aWlsZ80_Jy-A"},"source":["sns.set_style('ticks')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EYkD9BQsJy-A"},"source":["_ = sns.displot(x, kde=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bQULpJasJy-A"},"source":["When the normal distribution has a mean ($\\mu$) of zero and standard deviation ($\\sigma$) of one, as it does by default with the NumPy `normal()` method..."]},{"cell_type":"code","metadata":{"id":"kF2vyBE5Jy-A"},"source":["x.mean()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"scrolled":true,"id":"BBfTSFSLJy-A"},"source":["x.std()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"b36uZPgYJy-B"},"source":["...it is a **standard normal distribution** (a.k.a., standard Gaussian distribution or ***z*-distribution**), which can be denoted as $\\mathcal{N}(\\mu, \\sigma^2) = \\mathcal{N}(0, 1)$ (noting that $\\sigma^2 = \\sigma$ here because $1^2 = 1$)."]},{"cell_type":"markdown","metadata":{"id":"yghrt7snJy-B"},"source":["Normal distributions are by far the most common distribution in statistics and machine learning. They are typically the default option, particularly if you have limited information about the random process you're modeling, because:\n","\n","1. Normal distributions assume the greatest possible uncertainty about the random variable they represent (relative to any other distribution of equivalent variance). Details of this are beyond the scope of this tutorial.\n","2. Simple and very complex random processes alike are, under all common conditions, normally distributed when we sample values from the process. Since we sample data for statistical and machine learning models alike, this so-called **central limit theorem** (covered next) is a critically important concept."]},{"cell_type":"markdown","metadata":{"id":"i_o6lW48Jy-B"},"source":["### The Central Limit Theorem"]},{"cell_type":"markdown","metadata":{"id":"eGcUitnfJy-B"},"source":["To develop a functional understanding of the CLT, let's sample some values from our normal distribution:"]},{"cell_type":"code","metadata":{"id":"Zde5lo4oJy-B"},"source":["x_sample = np.random.choice(x, size=10, replace=False)\n","x_sample"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"AHPHlFhHJy-B"},"source":["The mean of a sample isn't always going to be close to zero with such a small sample:"]},{"cell_type":"code","metadata":{"id":"JlQhC9RrJy-C"},"source":["x_sample.mean()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Z5CMNU_MJy-C"},"source":["Let's define a function for generating **sampling distributions** of the mean of a given input distribution:"]},{"cell_type":"code","metadata":{"id":"RSUYBNRlJy-C"},"source":["def sample_mean_calculator(input_dist, sample_size, n_samples):\n","    sample_means = []\n","    for i in range(n_samples):\n","        sample = np.random.choice(input_dist, size=sample_size, replace=False)\n","        sample_means.append(sample.mean())\n","    return sample_means"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oI18ruGRJy-C"},"source":["sns.displot(sample_mean_calculator(x, 10, 10), color='green', kde=True)\n","_ = plt.xlim(-1.5, 1.5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xHGa07YxJy-C"},"source":["The more samples we take, the more likely that the sampling distribution of the means will be normally distributed:"]},{"cell_type":"code","metadata":{"id":"OPT9azuTJy-C"},"source":["sns.displot(sample_mean_calculator(x, 10, 1000), color='green', kde=True)\n","_ = plt.xlim(-1.5, 1.5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JhYHcSXSJy-C"},"source":["The larger the sample, the tighter the sample means will tend to be around the population mean:"]},{"cell_type":"code","metadata":{"id":"Bx3AvY5GJy-D"},"source":["sns.displot(sample_mean_calculator(x, 100, 1000), color='green', kde=True)\n","_ = plt.xlim(-1.5, 1.5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"EhsVc9EUJy-D"},"source":["sns.displot(sample_mean_calculator(x, 1000, 1000), color='green', kde=True)\n","_ = plt.xlim(-1.5, 1.5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uwW98i_MJy-E"},"source":["#### Sampling from a skewed distribution"]},{"cell_type":"code","metadata":{"id":"ZcAhyVlfJy-E"},"source":["s = st.skewnorm.rvs(10, size=10000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5agwbQu-Jy-E"},"source":["_ = sns.displot(s, kde=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1rU7N_XIJy-F"},"source":["_ = sns.displot(sample_mean_calculator(s, 10, 1000), color='green', kde=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Gf8z-zWnJy-F"},"source":["_ = sns.displot(sample_mean_calculator(s, 1000, 1000), color='green', kde=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"khgxd4kKJy-F"},"source":["#### Sampling from a multimodal distribution"]},{"cell_type":"code","metadata":{"id":"BwddoyHZJy-F"},"source":["m = np.concatenate((np.random.normal(size=5000), np.random.normal(loc = 4.0, size=5000)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b7-jIOsKJy-F"},"source":["_ = sns.displot(m, kde=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FgDJbhHNJy-G"},"source":["_ = sns.displot(sample_mean_calculator(m, 1000, 1000), color='green', kde=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"O1xAAvNsJy-G"},"source":["#### Sampling from uniform"]},{"cell_type":"markdown","metadata":{"id":"GPGNikM2Jy-G"},"source":["Even sampling from the highly non-normal uniform distribution, the sampling distribution comes out normal:"]},{"cell_type":"code","metadata":{"id":"P72RDUP_Jy-G"},"source":["u = np.random.uniform(size=10000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NnSaVMzzJy-G"},"source":["_ = sns.displot(u)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6ofAl-bXJy-G"},"source":["_ = sns.displot(sample_mean_calculator(u, 1000, 1000), color='green', kde=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CzsGWDvgJy-G"},"source":["Therefore, with large enough sample sizes, we can assume the sampling distribution of the means will be normally distributed, allowing us to apply statistical and ML models that are configured for normally distributed noise, which is often the default assumption.\n","\n","As an example, the \"*t*-test\" (covered shortly in *Intro to Stats*) allows us to infer whether two samples come from different populations (say, an experimental group that receives a treatment and a control group that receives a placebo). Thanks to the CLT, we can use this test even if we have no idea what the underlying distributions of the populations being tested are, which may be the case more frequently than not."]},{"cell_type":"markdown","metadata":{"id":"qdoH1h1cJy-H"},"source":["### z-scores"]},{"cell_type":"markdown","metadata":{"id":"amWbtyvMJy-H"},"source":["Assuming normally-distributed data, a z-score indicates how many standard deviations away from the mean a data point (say, $x_i$) is:\n","$$ z = \\frac{x_i-\\mu}{\\sigma} $$"]},{"cell_type":"markdown","metadata":{"id":"MR7WaPXEJy-H"},"source":["That is, the formula *standardizes* a given score $x_i$ to the (standard normal) *z*-distribution. (As we covered in *Probability & Information Theory*, you could standardize any normal distribution to a mean of zero and standard deviation of one by subtracting its original mean and then dividing by its original standard deviation.)"]},{"cell_type":"markdown","metadata":{"id":"hTAg32LLJy-H"},"source":["For example, let's say you get 85% on a CS101 exam. Sounds like a pretty good score and you did extremely well relative to your peers if the mean was 60% with a standard deviation of 10%:"]},{"cell_type":"code","metadata":{"id":"nCN9Lrc5Jy-H"},"source":["x_i = 85\n","mu = 60\n","sigma = 10"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oTVMKmYwJy-H"},"source":["x = np.random.normal(mu, sigma, 10000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JiCkR-4YJy-H"},"source":["sns.displot(x, color='gray')\n","ax.set_xlim(0, 100)\n","plt.axvline(mu, color='orange')\n","for v in [-3, -2, -1, 1, 2, 3]:\n","    plt.axvline(mu+v*sigma, color='olivedrab')\n","_ = plt.axvline(x_i, color='purple')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xWcxcmDlJy-I"},"source":["Your z-score is 2.5 standard deviations above the mean:"]},{"cell_type":"code","metadata":{"id":"63mlk5oEJy-I"},"source":["z = (x_i - mu)/sigma\n","z"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zh2fhCRDJy-I"},"source":["Or using our simulated class of 10k CS101 students:"]},{"cell_type":"code","metadata":{"id":"XwbcdpwYJy-I"},"source":["z = (x_i - np.mean(x))/np.std(x)\n","z"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Iw7tDGYCJy-J"},"source":["Less than one percent of the class outperformed you:"]},{"cell_type":"code","metadata":{"id":"EeVjREfhJy-J"},"source":["len(np.where(x > 85)[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pcJS3YIRJy-J"},"source":["100*69/10000"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"biFn-h76Jy-J"},"source":["np.percentile(x, 99)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cAZhJMenJy-J"},"source":["In contrast, if the mean score of your peers is 90 and the standard deviation is 2:"]},{"cell_type":"code","metadata":{"id":"fZj22bDfJy-J"},"source":["mu = 90\n","sigma = 2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6M0zc3PKJy-K"},"source":["y = np.random.normal(mu, sigma, 10000)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dj2CWs5kJy-K"},"source":["sns.displot(y, color='gray')\n","plt.axvline(mu, color='orange')\n","for v in [-3, -2, -1, 1, 2, 3]:\n","    plt.axvline(mu+v*sigma, color='olivedrab')\n","_ = plt.axvline(x_i, color='purple')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"z1b5fEFZJy-K"},"source":["Your z-score is 2.5 standard deviations *below* the mean (!):"]},{"cell_type":"code","metadata":{"id":"tnDjdaP2Jy-L"},"source":["z = (x_i - mu)/sigma\n","z"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VWZ7yvTZJy-L"},"source":["Or using our simulated class of 10k CS101 students:"]},{"cell_type":"code","metadata":{"id":"TgBIWccvJy-L"},"source":["z = (x_i - np.mean(y))/np.std(y)\n","z"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VR6cZOCVJy-L"},"source":["In which case, over 99% of the class outperformed you:"]},{"cell_type":"code","metadata":{"id":"7y4fP6U-Jy-L"},"source":["len(np.where(y > 85)[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KLH3RI-yJy-L"},"source":["100*9933/10000"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bFacTGGvJy-M"},"source":["A mere 67 folks attained worse:"]},{"cell_type":"code","metadata":{"id":"cgc7QrXxJy-M"},"source":["10000-9933"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aY7HDwhkJy-M"},"source":["np.percentile(y, 1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"aoqKrAPOJy-M"},"source":["A frequentist convention is to consider a data point that lies further than three standard deviations from the mean to be an **outlier**.\n","\n","It's a good idea to individually investigate outliers in your data as they may represent an erroneous data point (e.g., some data by accident, a data-entry error, or a failed experiment) that perhaps should be removed from further analysis (especially, as outliers can have an outsized impact on statistics including mean and correlation). It may even tip you off to a major issue with your data-collection methodology or your ML model that can be resolved or that you could have a unit test for."]},{"cell_type":"markdown","metadata":{"id":"VeTGuUJnJy-M"},"source":["**Exercises**\n","\n","1. You clean and jerk 100kg in a weightlifting competition. The mean C&J weight at the competition is 100kg. What's your z-score for the C&J?\n","2. You snatch 100kg in the same competition. The mean snatch weight is 80kg with a standard deviation of 10kg. What's your z-score for the snatch?\n","3. In olympic weightlifting, your overall score is the sum total of your C&J and snatch weights. The mean of these totals across competitors is 180kg with a standard deviation of 5kg. What's your overall z-score in the competition?\n","\n","**Spoiler alert**: Solutions below"]},{"cell_type":"markdown","metadata":{"id":"NlFugQ1VJy-P"},"source":["**Solutions**\n","1. zero\n","2. two\n","3. four (you may have won the meet!)"]},{"cell_type":"markdown","metadata":{"id":"PNB9TnHgJy-P"},"source":["### *p*-values"]},{"cell_type":"markdown","metadata":{"id":"JFpWDfhPJy-P"},"source":["These quantify the *p*robability that a given observation would occur by chance alone.\n","\n","For example, we saw above that with our simulated 10k exam results, only 69 folks attained a *z*-score above 2.5 and only 67 (=10000-9993) attained a *z*-score below -2.5. Thus, if we were to randomly sample one of the 10k CS101 exam results, we would expect it to be outside of 2.5 (i.e., +/- 2.5) standard deviations only 1.36% of the time:\n","$$ \\frac{69+67}{10000} = 0.0136 = 1.36\\% $$"]},{"cell_type":"markdown","metadata":{"id":"uZGJqELvJy-Q"},"source":["Equivalent to increasing our CS101 class size from 10k toward infinity, the probability of a score being further than 2.5 standard deviations from the mean of a normal distribution can be determined with the distribution's *cumulative distribution function* (CDF):"]},{"cell_type":"code","metadata":{"id":"5s-ySScTJy-Q"},"source":["p_below = st.norm.cdf(-2.5)\n","p_below"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u62I9OjtJy-Q"},"source":["p_below*10000"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0-AGIPvRJy-Q"},"source":["sns.displot(y, color='gray')\n","_ = plt.axvline(mu-2.5*sigma, color='blue')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nI18Ajo-Jy-Q"},"source":["st.norm.cdf(2.5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XeX3OfokJy-Q"},"source":["p_above = 1-st.norm.cdf(2.5)\n","p_above"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GQ4fjiJZJy-Q"},"source":["p_above*10000"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"e8-QvwuNJy-R"},"source":["sns.displot(y, color='gray')\n","_ = plt.axvline(mu+2.5*sigma, color='blue')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GYllm_UaJy-R"},"source":["p_outside = p_below + p_above\n","p_outside"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"44Cq4EtKJy-R"},"source":["p_outside*10000"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6zdZEbPmJy-R"},"source":["sns.displot(y, color='gray')\n","plt.axvline(mu+2.5*sigma, color='blue')\n","_ = plt.axvline(mu-2.5*sigma, color='blue')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"I0TSl17uJy-R"},"source":["In other words, assuming a normal distribution, the probability (the *p*-value) of a sampled value being at least 2.5 standard deviations away from the mean by chance alone is $p \\approx .0124$."]},{"cell_type":"markdown","metadata":{"id":"4syN_b9hJy-R"},"source":["The frequentist convention is that if a *p*-value is less than .05, we can say that it is a \"statistically significant\" observation. We typically denote this significance threshold with $\\alpha$, e.g., $\\alpha = .05$.\n","\n","For example, with a fair coin, the probability of throwing six heads *or* six tails in a six-coin-flip experiment is 0.03125 ($p = 0.015625$ for *either of* six heads or six tails). Refer back to the `coinflip_prob()` method from the [*Probability* notebook](https://github.com/jonkrohn/ML-foundations/blob/master/notebooks/5-probability.ipynb) for proof.\n","\n","If a friend of yours hands you a coin, the **null hypothesis** (the baseline assumed by the fair-toss distribution) would be that the coin is fair. If you test this coin by flipping it six times and it comes up heads on all six or tails on all six, this observation would suggest that you should *reject the null hypothesis* because chance alone would facilitate such an observation less than 5% of the time, i.e., $p < .05$."]},{"cell_type":"markdown","metadata":{"id":"KMt6x7BQJy-S"},"source":["The *z*-scores corresponding to $\\alpha = .05$ can be obtained from the normal distribution's *percent point function* (PPF), which facilitates the inverse of the CDF. To capture 95% of the values around the mean, we leave 2.5% at the bottom of the distribution and 2.5% at the top:"]},{"cell_type":"code","metadata":{"id":"y3ZqdV9WJy-S"},"source":["st.norm.ppf(.025)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VJGV5em-Jy-S"},"source":["st.norm.ppf(.975)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RObWUObeJy-S"},"source":["Thus, at the traditional $\\alpha = .05$, a sampled value with *z*-score less than -1.96 or greater than 1.96 would be considered statistically significant."]},{"cell_type":"code","metadata":{"id":"a7yRDzN-Jy-S"},"source":["sns.displot(y, color='gray')\n","plt.axvline(mu+1.96*sigma, color='darkred')\n","_ = plt.axvline(mu-1.96*sigma, color='darkred')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"esOQED5oqLSL"},"source":["With a stricter threshold, say $\\alpha = .01$:"]},{"cell_type":"code","metadata":{"id":"2J1meOTUqIcG"},"source":["st.norm.ppf(.005)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DYqpw9SpqQNc"},"source":["st.norm.ppf(.995)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H5JqPoSdqSlp"},"source":["sns.displot(y, color='gray')\n","\n","plt.axvline(mu+1.96*sigma, color='darkred')\n","plt.axvline(mu-1.96*sigma, color='darkred')\n","\n","plt.axvline(mu+2.56*sigma, color='black')\n","_ = plt.axvline(mu-2.56*sigma, color='black')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jOK4j775Jy-S"},"source":["(Time-permitting, a discussion of two-tailed vs one-tailed *p*-value tests would be informative here.)"]},{"cell_type":"markdown","metadata":{"id":"eUO9cKo7Jy-S"},"source":["**Exercises**\n","\n","1. What are the *p*-values associated with your weightlifting results from the three preceding exercises?\n","2. With the standard $\\alpha = .05$, which of the three weightlifting results are \"statistically significant\"?\n","\n","**Spoiler alert**: Solutions below"]},{"cell_type":"markdown","metadata":{"id":"CWQrcJOYJy-T"},"source":["**Solutions**"]},{"cell_type":"markdown","metadata":{"id":"OGa9SFweJy-T"},"source":["1a. This result is at the mean, which is also the median for a normal distribution; exactly half of the values are above as they are below. This corresponds to the highest possible $p$-value, $p=1$, because any value in the distribution is guaranteed to be above it or below it:"]},{"cell_type":"code","metadata":{"id":"ZQKATPPkJy-T"},"source":["p_below = st.norm.cdf(0)\n","p_below"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dmqPNWSqJy-U"},"source":["p_above = 1-st.norm.cdf(0)\n","p_above"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fn6BF8RbJy-U"},"source":["p_below + p_above"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RWdGMzQ0Jy-U"},"source":["More generally:"]},{"cell_type":"code","metadata":{"id":"BlWSYNYEJy-U"},"source":["def p_from_z(my_z):\n","    return 2 * st.norm.cdf(-abs(my_z))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V-wR64UTJy-V"},"source":["p_from_z(0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"jXZzO-nUJy-V"},"source":["1b. The probability of a value being below $z = -2$ is:"]},{"cell_type":"code","metadata":{"id":"kmVVA_Z-Jy-V"},"source":["p_below = st.norm.cdf(-2)\n","p_below"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"uqHYBMw9Jy-V"},"source":["...and the probability of a value being above $z=2$ is the same:"]},{"cell_type":"code","metadata":{"id":"UFk8qbOYJy-V"},"source":["p_above = 1-st.norm.cdf(2)\n","p_above"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LYa0XOgUJy-V"},"source":["Therefore, the *p*-value -- the probability that a value is below $z=-2$ or above $z=2$ -- is:"]},{"cell_type":"code","metadata":{"id":"qur84VgxJy-W"},"source":["p_below + p_above"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3RJV4nIJJy-W"},"source":["p_from_z(2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xwimaURKJy-W"},"source":["1c. Following the same calculations as we did for 1b, the *p*-value for an observation 4 standard deviations away from the mean is:"]},{"cell_type":"code","metadata":{"id":"S8NL5ziVJy-W"},"source":["p_from_z(4)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ePMcdC6AJy-W"},"source":["...which is about 0.0000633:"]},{"cell_type":"code","metadata":{"id":"blFPCwJLJy-W"},"source":["0.0000633"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"5DcLWbVQJy-X"},"source":["(Incidentally, very small *p* values are often reported as **negative log *P*** values as these are much easier to read...)"]},{"cell_type":"code","metadata":{"id":"vpmwsw5OJy-X"},"source":["-np.log10(6.33e-05)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xzmOYl6xJy-X"},"source":["2. The absolute value of the *z*-score for your snatch as well as your combined score is greater than 1.96 so they're both \"statistically significant\". Your performance on the clean and jerk could not have been less significant!"]},{"cell_type":"markdown","metadata":{"id":"HA3mbmo4Jy-X"},"source":["### Comparing Means with *t*-tests"]},{"cell_type":"markdown","metadata":{"id":"OBARGrHjJy-X"},"source":["Where *z*-scores apply to *individual values* only, *t*-tests enables us to compare (the mean of) a sample of *multiple values* to a reference mean."]},{"cell_type":"markdown","metadata":{"id":"FFS2GmrxJy-X"},"source":["#### Student's Single-Sample *t*-test"]},{"cell_type":"markdown","metadata":{"id":"OEkXJ94nJy-X"},"source":["Named after William Sealy Gosset, an Oxford-trained scientist and mathematician, who became a stout yield statistician for Guinness in Dublin (from 1899 to his fatal heart attack in 1937 shortly after being promoted to head brewer). Alongside sabbaticals in Karl Pearson's UCL Biometric Laboratory, Gosset published under the pseudonym Student (including on the *t*-test, starting in 1908) as it was against Guinness policy to publish."]},{"cell_type":"markdown","metadata":{"id":"ShlPrRtVJy-X"},"source":["Recalling the formula for calculating a *z*-score:\n","$$ z = \\frac{x_i-\\mu}{\\sigma} $$"]},{"cell_type":"markdown","metadata":{"id":"BoXBaK4WJy-Y"},"source":["The **single-sample *t*-test** is a variation on the theme and is defined by:\n","$$ t = \\frac{\\bar{x} - \\mu_0}{s_{\\bar{x}}} $$\n","Where:\n","* $\\bar{x}$ is the sample mean\n","* $\\mu_0$ is a reference mean, e.g., known population mean or \"null hypothesis\" mean\n","* $s_{\\bar{x}}$ is the sample standard error"]},{"cell_type":"markdown","metadata":{"id":"lqnCyHq6Jy-Y"},"source":["Let's say you're the head brewer at Guinness. Your baseline brewing process yields 50L of stout. Using a new genetically-modified yeast, you obtain the following yields (all in liters) in four separate experiments:"]},{"cell_type":"code","metadata":{"id":"2Lo8ozG5Jy-Y"},"source":["x = [48, 50, 54, 60]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oov4H2IvJy-Y"},"source":["We can obtain the *t*-statistic for this sample as follows:"]},{"cell_type":"code","metadata":{"id":"JR_0S3KyJy-Y"},"source":["xbar = np.mean(x)\n","xbar"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2OFpbkOlJy-Y"},"source":["sx = st.sem(x)\n","sx"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YSX1KZ2nJy-Y"},"source":["t = (xbar-50)/sx\n","t"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1KAvQMTUJy-Z"},"source":["We can convert the *t*-value into a *p*-value using Student's *t*-distribution (similar to the normal *z*-distribution, but varies based on number of data points in sample; see [here](https://en.wikipedia.org/wiki/Student%27s_t-distribution) for more detail):"]},{"cell_type":"code","metadata":{"id":"yqled1DnJy-Z"},"source":["def p_from_t(my_t, my_n):\n","    return 2 * st.t.cdf(-abs(my_t), my_n-1) # 2nd arg to t.cdf() is \"degrees of freedom\""],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b_l4p5YwJy-Z"},"source":["p_from_t(t, len(x))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"x0BPjLnwJy-Z"},"source":["(An illustration of **degrees of freedom**: If we know the mean of the array `x`, three of its four values can vary freely. That is, if we know three of the values in the array, the fourth has no \"freedom\"; it must be a specific value. Thus, the most common situation with statistical tests is that we have *n*-1 degrees of freedom.)"]},{"cell_type":"markdown","metadata":{"id":"z4-b2kfJJy-Z"},"source":["For everyday usage, however, we can rely on the SciPy `ttest_1samp()` method:"]},{"cell_type":"code","metadata":{"id":"ZhHgjupDJy-Z"},"source":["st.ttest_1samp(x, 50)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7yB8zKkOJy-a"},"source":["#### Welch's Independent *t*-test"]},{"cell_type":"markdown","metadata":{"id":"LHhP5rBJJy-a"},"source":["In ordinary circumstances, if we have two samples whose means we'd like to compare, we use an **independent *t*-test**."]},{"cell_type":"code","metadata":{"id":"43W0pLTKJy-a"},"source":["penguins = sns.load_dataset('penguins').dropna() # some rows are missing data"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uzhbtPXzJy-a"},"source":["penguins"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QiN2SlJPJy-a"},"source":["np.unique(penguins.species, return_counts=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oYNbPQ6aJy-a"},"source":["adelie = penguins[penguins.species == 'Adelie']"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"izX0Q8CXJy-b"},"source":["adelie"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nO8GBeloJy-b"},"source":["np.unique(adelie.island, return_counts=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7OYSleCKJy-b"},"source":["np.unique(adelie.sex, return_counts=True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lsOZu9s9Jy-b"},"source":["_ = sns.boxplot(x='island', y='body_mass_g', hue='sex', data=adelie)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2VaIivkrJy-b"},"source":["Mass doesn't appear to vary by island, so we can feel comfortable grouping the data together by island. Weight does, however, appear to vary by sex so let's take a closer look:"]},{"cell_type":"code","metadata":{"id":"kx904jk5Jy-b"},"source":["f = adelie[adelie.sex == 'Female']['body_mass_g'].to_numpy()/1000\n","f"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s_geKklTJy-c"},"source":["m = adelie[adelie.sex == 'Male']['body_mass_g'].to_numpy()/1000\n","m"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XfpowzzfJy-c"},"source":["fbar = f.mean()\n","fbar"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tMzJ6hpTJy-c"},"source":["mbar = m.mean()\n","mbar"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8y2BYap9Jy-c"},"source":["To quantify whether males weigh significantly more than females, we can use the **Welch *t*-test**, devised by the 20th c. British statistician Bernard Lewis Welch:\n","$$ t = \\frac{\\bar{x} - \\bar{y}}{\\sqrt{\\frac{s^2_x}{n_x} + \\frac{s^2_y}{n_y}}} $$\n","Where:\n","* $\\bar{x}$ and $\\bar{y}$ are the sample means\n","* $s^2_x$ and $s^2_y$ are the sample variances\n","* $n_x$ and $n_y$ are the sample sizes"]},{"cell_type":"markdown","metadata":{"id":"8W_RnYsCJy-d"},"source":["**N.B.**: Student's independent *t*-test is markedly more popular than Welch's, but Student's assumes equal population variances (i.e., $\\sigma^2_x \\approx \\sigma^2_y$), making it less robust. In case you're curious, Student's formula is the same as Welch's, except that it uses a pooled variance $s^2_p$ in place of individual sample variances ($s^2_x$ and $s^2_y$). You can read more about it [here](https://en.wikipedia.org/wiki/Student%27s_t-test#Independent_two-sample_t-test)."]},{"cell_type":"code","metadata":{"id":"cl3g6CmrJy-d"},"source":["sf = f.var(ddof=1)\n","sm = m.var(ddof=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"qCThyw6ZJy-d"},"source":["nf = f.size\n","nm = m.size"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7oApFUC_Jy-d"},"source":["t = (fbar-mbar)/(sf/nf + sm/nm)**(1/2)\n","t"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"04QiVOYcJy-d"},"source":["Degrees of freedom for calculating the *p*-value are estimated using the [Welch–Satterthwaite equation](https://en.wikipedia.org/wiki/Welch–Satterthwaite_equation), which we won't detail but is defined as:"]},{"cell_type":"code","metadata":{"id":"GXyapE2pJy-d"},"source":["def ws_eqn(sx, sy, nx, ny):\n","    return (sx / nx + sy / ny)**2 / (sx**2 / (nx**2 * (nx - 1)) + sy**2 / (ny**2 * (ny - 1)))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bhiKHnxXJy-d"},"source":["df = ws_eqn(sf, sm, nf, nm)\n","df"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uA-751AVJy-e"},"source":["p = 2 * st.t.cdf(-abs(t), df) # or p_from_t(t, df+1)\n","p"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yKrUZU9oSylp"},"source":["p_from_t(t, df+1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Ug3l4vNJy-e"},"source":["-np.log10(p)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Fj1klL8RJy-e"},"source":["st.ttest_ind(f, m, equal_var=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mz_5pa7WJy-e"},"source":["#### Student's Paired *t*-test"]},{"cell_type":"markdown","metadata":{"id":"kaomPLqyJy-e"},"source":["Occasionally, we have two vectors where each element in vector *x* has a corresponding element in vector *y*.\n","\n","For example, we could run an experiment where Alzheimer's disease patients receive a drug on one day (experimental condition) and a sugar pill placebo (control condition) on another day. We can then measure the patients' forgetfulness on both days to test whether the drug has a significant impact on memory.\n","\n","For a given sample size, such a paired *t*-test is more powerful relative to an unpaired (independent) *t*-test because the variance of *x* is directly related to the variance in *y*: A severe Alzheimer's patient will tend to be relatively forgetful on both days, while a mild Alzheimer's patient will tend to be relatively unforgetful on both days. With paired samples, we capture this power by comparing the *difference* between *x* and *y*, e.g., the difference in forgetfulness for a given patient when given the drug relative to when given the sugar pill.\n","\n","In contrast, consider the penguin dataset, wherein we wouldn't be able to obviously pair a given male penguin with a correponding female penguin. Or consider a situation where we provide a drug to one set of Alzheimer's patients while we provide a placebo to an entire different (an independent) group of patients. Indeed, with an independent *t*-test we could even have different sample sizes in the two groups whereas this is impossible with a paired *t*-test."]},{"cell_type":"markdown","metadata":{"id":"mRknH0rPJy-e"},"source":["Here's an example:"]},{"cell_type":"code","metadata":{"id":"I5OLpqPvJy-f"},"source":["exercise = sns.load_dataset('exercise')\n","exercise"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NYi9P-nIJy-f"},"source":["There are 30 people in the dataset, with their pulse taken at three different time points in an experiment (i.e, after one, 15, and 30 minutes). Ten people were assigned to each of three activity groups:"]},{"cell_type":"code","metadata":{"id":"8ZmmIMPEJy-f"},"source":["np.unique(exercise.kind, return_counts=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"49PjJ7U2Jy-g"},"source":["Within each of those activity groups, half of the participants are on a low-fat diet while the other half are on a no-fat diet:"]},{"cell_type":"code","metadata":{"id":"0L3Ez96QJy-g"},"source":["np.unique(exercise.diet, return_counts=True)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"CLMGo8KNJy-g"},"source":["For simplicity, let's only consider one of the six experimental groups, say the walking, no-fat dieters:"]},{"cell_type":"code","metadata":{"id":"i6a8K8bwJy-g"},"source":["walk_no = exercise[(exercise.diet == 'no fat') & (exercise.kind == 'walking')]\n","walk_no"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wkNKuo-UJy-g"},"source":["(Note how participant 16 has a relatively low heart rate at all three timepoints, whereas participant 20 has a relatively high heart rate at all three timepoints.)"]},{"cell_type":"code","metadata":{"id":"8mJ_Bzz5Jy-g"},"source":["_ = sns.boxplot(x='time', y='pulse', data=walk_no)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"u8IgFK6KJy-h"},"source":["min1 = walk_no[walk_no.time == '1 min']['pulse'].to_numpy()\n","min1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o_LOW8VFJy-h"},"source":["min1.mean()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9H8GdBZoJy-h"},"source":["min15 = walk_no[walk_no.time == '15 min']['pulse'].to_numpy()\n","min15"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D0QZlQl8Jy-h"},"source":["min15.mean()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rkCzRQnwJy-h"},"source":["min30 = walk_no[walk_no.time == '30 min']['pulse'].to_numpy()\n","min30"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yWmnohV5Jy-h"},"source":["min30.mean()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2IPyQq9iJy-i"},"source":["(With paired samples, we can plot the values in a scatterplot, which wouldn't make any sense for independent samples, e.g.:)"]},{"cell_type":"code","metadata":{"id":"rNPRuNaDJy-i"},"source":["sns.scatterplot(x=min1, y=min15)\n","plt.title('Heart rate of no-fat dieters (beats per minute)')\n","plt.xlabel('After 1 minute walking')\n","_ = plt.ylabel('After 15 minutes walking')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7ah_p_eVJy-i"},"source":["To assess whether the mean heart rate varies significantly after one minute of walking relative to after 15 minutes, we can use Student's **paired-sample** (a.k.a., **dependent**) *t*-test:\n","$$ t = \\frac{\\bar{d} - \\mu_0}{s_\\bar{d}} $$\n","Where:\n","* $d$ is a vector of the differences between paired samples $x$ and $y$\n","* $\\bar{d}$ is the mean of the differences\n","* $\\mu_0$ will typically be zero, meaning the null hypothesis is that there is no difference between $x$ and $y$\n","* $s_\\bar{d}$ is the standard error of the differences"]},{"cell_type":"markdown","metadata":{"id":"5w618u07Jy-i"},"source":["(Note how similar to single-sample *t*-test formula.)"]},{"cell_type":"code","metadata":{"id":"GSaoJW5IJy-i"},"source":["d = min15 - min1\n","d"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"elsBVr9AJy-i"},"source":["dbar = d.mean()\n","dbar"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"yD8KOL3EJy-j"},"source":["sd = st.sem(d)\n","sd"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uJglNA37Jy-j"},"source":["t = (dbar-0)/sd\n","t"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Mcq9LALJy-j"},"source":["p_from_t(t, d.size)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7SZqWeDSJy-j"},"source":["st.ttest_rel(min15, min1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"oZy5OPUcJy-j"},"source":["In contrast, if we were to put the same values into an independent *t*-test..."]},{"cell_type":"code","metadata":{"id":"-Zxlb9MjJy-j"},"source":["st.ttest_ind(min15, min1, equal_var=False)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3Qq6CwH2Jy-k"},"source":["#### Machine Learning Examples"]},{"cell_type":"markdown","metadata":{"id":"QKsOq0Q2Jy-k"},"source":["* Single-sample: Does my stochastic model tend to be more accurate than an established benchmark?\n","* Independent samples: Does my model have unwanted bias in it, e.g., do white men score higher than other demographic groups with HR model?\n","* Paired samples: Is new TensorFlow.js model significantly faster? (paired by browser / device)"]},{"cell_type":"markdown","metadata":{"id":"bVKNILucJy-k"},"source":["**Exercises**\n","1. You run four additional experiments with your GMO brewing yeast and now have the following eight stout yields: `[48, 50, 54, 60, 49, 55, 59, 62]`. What is the *t*-statistic and is it significantly different from the 50L-yielding baseline process?\n","2. Does the flipper length of Adélie penguins from Dream island vary significantly by sex?\n","2. Was the heart rate of low-fat dieters different after one minute of rest relative to after 15 minutes of rest?\n","\n","**Spoiler alert**: Solutions below"]},{"cell_type":"markdown","metadata":{"id":"EBk3r0Q_Jy-k"},"source":["**Solutions**\n","1. The GMO yeast yields a mean of 54.6L, which is significantly more stout than the baseline process, *t*(7) = 2.45, $p < .05$."]},{"cell_type":"code","metadata":{"id":"h8K4hhf0Jy-l"},"source":["st.ttest_1samp([48, 50, 54, 60, 49, 55, 59, 62], 50)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2uSTVC7PJy-l"},"source":["2. On Dream island, the flippers of male Adélie penguins (191.9 mm) are significantly longer than those of females (187.9 mm), *t* = 2.4, *p* < .05."]},{"cell_type":"code","metadata":{"id":"wVvnWp-nJy-l"},"source":["_ = sns.boxplot(x='island', y='flipper_length_mm', hue='sex', data=adelie)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6NQmdBuxJy-l"},"source":["f = adelie[(adelie.sex == 'Female') & (adelie.island == 'Dream')]['flipper_length_mm'].to_numpy()\n","m = adelie[(adelie.sex == 'Male') & (adelie.island == 'Dream')]['flipper_length_mm'].to_numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"087RkPOgJy-l"},"source":["f.mean()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bTpJdjU-Jy-m"},"source":["m.mean()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gDK9V7pTJy-m"},"source":["tp = st.ttest_ind(f, m, equal_var=False)\n","tp"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3xPeCr7jJy-m"},"source":["tp.pvalue"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"7sGchromJy-m"},"source":["3. The heart rate of low-fat dieters did not change significantly after one minute of rest (88.6 bpm) relative to after 15 minutes of rest (89.6 bpm), *t*=2.2, *p* = .09."]},{"cell_type":"code","metadata":{"id":"ZL0ZiOjaJy-n"},"source":["rest_lo = exercise[(exercise.diet == 'low fat') & (exercise.kind == 'rest')]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L6TswIXmcJjM"},"source":["_ = sns.boxplot(x='time', y='pulse', data=rest_lo)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DrFCIU5oJy-n"},"source":["min1 = rest_lo[rest_lo.time == '1 min']['pulse'].to_numpy()\n","min1.mean()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LKBkGXAeJy-n"},"source":["min15 = rest_lo[rest_lo.time == '15 min']['pulse'].to_numpy()\n","min15.mean()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H0s2me3RJy-n"},"source":["st.ttest_rel(min15, min1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MkAeEk4PJy-n"},"source":["### Confidence Intervals"]},{"cell_type":"markdown","metadata":{"id":"8IdBn7D6Jy-n"},"source":["When examining sample means as we have been for the *t*-test, a useful statistical tool is the **confidence interval** (CI), which we for example often see associated with polling results when there's an upcoming election. CIs allow us to make statements such as \"there is a 95% chance that the population mean lies within this particular range of values\"."]},{"cell_type":"markdown","metadata":{"id":"Nk_XsypkJy-o"},"source":["We can calculate a CI by rearranging the *z*-score formula:\n","$$ \\text{C.I.} = \\bar{x} \\pm z \\frac{s}{\\sqrt{n}} $$\n","Where:\n","* $\\bar{x}$ is the sample mean\n","* $s$ is the sample standard deviation\n","* $n$ is the sample size\n","* $z$ corresponds to a *z*-score threshold (e.g., the most common 95% CI is $z \\pm 1.960$; other popular ones are the 90% CI at $z \\pm 1.645$ and the 99% CI at $z \\pm 2.576$)"]},{"cell_type":"markdown","metadata":{"id":"0FZRFc_tJy-o"},"source":["For example, to find the 95% confidence interval for the true mean yield of our GMO yeast:"]},{"cell_type":"code","metadata":{"id":"JoiW9cZmJy-o"},"source":["x = np.array([48, 50, 54, 60, 49, 55, 59, 62])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sF4Oxn-KJy-o"},"source":["xbar = x.mean()\n","s = x.std()\n","n = x.size"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZI7gS9CUJy-o"},"source":["z = 1.96"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ghR6ySR5Jy-o"},"source":["def CIerr_calc(my_z, my_s, my_n):\n","    return my_z*(my_s/my_n**(1/2))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zUnONG0FJy-o"},"source":["CIerr = CIerr_calc(z, s, n)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wJy0oR0PhPe4"},"source":["CIerr"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eUgR-OumJy-o"},"source":["xbar + CIerr"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dcSAZBArJy-p"},"source":["xbar - CIerr"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9ILUj-4vJy-p"},"source":["Therefore, there's a 95% chance that the true mean yield of our GMO yeast lies in the range of 51.2 to 58.1 liters. Since this CI doesn't overlap with the established baseline mean of 50L, this corresponds to stating that the GMO yield is significantly greater than the baseline where $\\alpha = .05$, as we already determined:"]},{"cell_type":"code","metadata":{"id":"XzL2woRiJy-p"},"source":["fig, ax = plt.subplots()\n","plt.ylabel('Stout Yield (L)')\n","plt.grid(axis='y')\n","ax.errorbar(['GMO'], [xbar], [CIerr], fmt='o', color='green')\n","_ = ax.axhline(50, color='orange')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VNhOA10fJy-p"},"source":["Similarly, we can compare several sample means with CIs. Using our penguins again:"]},{"cell_type":"code","metadata":{"id":"IRyeB9TQJy-p"},"source":["fCIerr = CIerr_calc(z, sf, nf)\n","mCIerr = CIerr_calc(z, sm, nm)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b_xJOcVYJy-p"},"source":["fig, ax = plt.subplots()\n","plt.title('Adélie Penguins')\n","plt.ylabel('Weight (kg)')\n","plt.grid(axis='y')\n","_ = ax.errorbar(['female', 'male'], [fbar, mbar], [fCIerr, mCIerr],\n","                fmt='.', color='green')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"xNBdDo13Jy-p"},"source":["The CIs are nowhere near overlapping, corresponding to the extremely significant (-log*P* $\\approx 25$) difference in penguin weight."]},{"cell_type":"markdown","metadata":{"id":"__UUzJKhJy-q"},"source":["In contrast, the CIs for female penguins from the three islands..."]},{"cell_type":"code","metadata":{"id":"-I5QSiz1Jy-q"},"source":["t = adelie[(adelie.sex == 'Female') & (adelie.island == 'Torgersen')]['body_mass_g'].to_numpy()/1000\n","b = adelie[(adelie.sex == 'Female') & (adelie.island == 'Biscoe')]['body_mass_g'].to_numpy()/1000\n","d = adelie[(adelie.sex == 'Female') & (adelie.island == 'Dream')]['body_mass_g'].to_numpy()/1000"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bSl86d2CJy-q"},"source":["means = [t.mean(), b.mean(), d.mean()]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"oUW9l3FeJy-q"},"source":["s_t, sb, sd = t.var(ddof=1), b.var(ddof=1), d.var(ddof=1) # s_t to disambiguate stats package"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"d6OLKRJGJy-q"},"source":["nt, nb, nd = t.size, b.size, d.size"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0JVMu-zmJy-q"},"source":["CIerrs = [CIerr_calc(z, s_t, nt), CIerr_calc(z, sb, nb), CIerr_calc(z, sd, nd)]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"s3-yrx0IJy-q"},"source":["fig, ax = plt.subplots()\n","plt.title('Female Adélie Penguins')\n","plt.ylabel('Weight (kg)')\n","plt.grid(axis='y')\n","_ = ax.errorbar(['Torgersen', 'Biscoe', 'Dream'], means, CIerrs,\n","                fmt='o', color='green')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c_mv-Q3oJy-q"},"source":["### ANOVA: Analysis of Variance"]},{"cell_type":"markdown","metadata":{"id":"y5I-yTOAJy-r"},"source":["**Analysis of variance** (ANOVA) enables us to compare more than two samples (e.g., all three islands in the case of penguin weight) in a single statistical test."]},{"cell_type":"markdown","metadata":{"id":"0jAvOMmwJy-r"},"source":["To apply ANOVA, we must make three assumptions:\n","1. Independent samples\n","2. Normally-distributed populations\n","3. *Homoscedasticity*: Population standard deviations are equal"]},{"cell_type":"markdown","metadata":{"id":"2MdpZtTcJy-r"},"source":["While not especially complicated under the hood (you can dig into the formulae [here](https://en.wikipedia.org/wiki/Analysis_of_variance#Logic)), ANOVA might be the least widely-applicable topic within *Intro to Stats* to ML so in the interest of time, we'll skip straight to the Python code:"]},{"cell_type":"code","metadata":{"id":"rlCB3yAeJy-r"},"source":["st.f_oneway(t, b, d)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1iiTah2uJy-r"},"source":["### Pearson Correlation Coefficient"]},{"cell_type":"markdown","metadata":{"id":"Q-yMENfQJy-r"},"source":["If we have two vectors of the same length, $x$ and $y$, where each element of $x$ is paired with the corresponding element of $y$, **covariance** provides a measure of how related the variables are to each other:\n","$$ \\text{cov}(x, y) = \\frac{\\sum_{i=1}^n (x_i - \\bar{x})(y_i - \\bar{y}) }{n} $$"]},{"cell_type":"markdown","metadata":{"id":"K1OsRpDXJy-r"},"source":["A drawback of covariance is that it confounds the relative scale of two variables with a measure of the variables' relatedness. **Correlation** builds on covariance and overcomes this drawback via rescaling, thereby measuring (linear) relatedness exclusively. Correlation is much more common because of this difference.\n","\n","The correlation coefficient (developed by Karl Pearson in the 20th c. though known in the 19th c.) is often denoted with $r$ or $\\rho$ and is defined by:\n","$$ \\rho_{x,y} = \\frac{\\text{cov}(x,y)}{\\sigma_x \\sigma_y} $$"]},{"cell_type":"code","metadata":{"id":"YZvTV6dAJy-r"},"source":["iris = sns.load_dataset('iris')\n","iris"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UnrJjhx0Jy-s"},"source":["x = iris.sepal_length\n","y = iris.petal_length"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hPE0FcBTJy-s"},"source":["sns.set_style('darkgrid')"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VVEf2DoBJy-s"},"source":["_ = sns.scatterplot(x=x, y=y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-BMruFjYJy-s"},"source":["n = iris.sepal_width.size"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OWjlIrqxJy-s"},"source":["xbar, ybar = x.mean(), y.mean()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J1K3jIeoJy-s"},"source":["product = []\n","for i in range(n):\n","    product.append((x[i]-xbar)*(y[i]-ybar))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z6o2_0xqJy-s"},"source":["cov = sum(product)/n\n","cov"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sNrADHruJy-t"},"source":["r = cov/(np.std(x)*np.std(y))\n","r"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TrfrcRSRJy-t"},"source":["We reached this point in *Probability*. Now, as for how to determine a *p*-value, we first calculate the *t*-statistic...\n","$$ t = r \\sqrt{\\frac{n-2}{1-r^2}} $$\n","\n","(This formula standardizes the correlation coefficient, taking into account the sample size *n* and the strength of the relationship *r*, to produce a *t*-statistic that follows [Student's *t*-distribution](https://en.wikipedia.org/wiki/Student%27s_t-distribution).)"]},{"cell_type":"code","metadata":{"id":"jriXvRsfJy-t"},"source":["t = r*((n-2)/(1-r**2))**(1/2)\n","t"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0478RJCXJy-t"},"source":["...which we can convert to a *p*-value as we've done several times above:"]},{"cell_type":"code","metadata":{"id":"ZyPsXHR3Jy-t"},"source":["p = p_from_t(t, n-1)\n","p"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"duzLjTGmJy-t"},"source":["-np.log10(p)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"MKrnu6iZJy-u"},"source":["This confirms that iris sepal length's positive correlation with petal length is (extremely!) statistically significant."]},{"cell_type":"markdown","metadata":{"id":"mBNh5oBPJy-u"},"source":["All of the above can be done in a single line with SciPy's `pearsonr()` method:"]},{"cell_type":"code","metadata":{"id":"wsCA5H9zJy-u"},"source":["st.pearsonr(x, y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"-_UNB98rJy-u"},"source":["And, for reference, here's a correlation that is not significant ($r \\approx 0$):"]},{"cell_type":"code","metadata":{"id":"hM12XzeQJy-u"},"source":["_ = sns.scatterplot(x=iris.sepal_length, y=iris.sepal_width)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"G9ILTCGkJy-u"},"source":["st.pearsonr(iris.sepal_length, iris.sepal_width)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RpgrOtddJy-u"},"source":["### The Coefficient of Determination"]},{"cell_type":"markdown","metadata":{"id":"QpwA6dZIJy-v"},"source":["...also known as $r^2$, this is the proportion of variance in one variable explained by another.\n","\n","It can range from 0 to 1 and it is simply the square of the Pearson $r$:"]},{"cell_type":"code","metadata":{"id":"1Dq-99gWJy-v"},"source":["rsq = r**2\n","rsq"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"2dsLJOK-Jy-v"},"source":["In this case, it indicates that 76% of the variance in iris petal length can be explained by sepal length. (This is easier to understand where one variable could straightforwardly drive variation in the other; more on that in Segment 2.)"]},{"cell_type":"markdown","metadata":{"id":"Dyt_nWhZJy-v"},"source":["For comparison, only 1.4% of the variance in sepal width can be explained by sepal length:"]},{"cell_type":"code","metadata":{"id":"jqoRP7sIJy-v"},"source":["st.pearsonr(iris.sepal_length, iris.sepal_width)[0]**2"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"DAzNEPfMJy-v"},"source":["### Correlation vs Causation"]},{"cell_type":"markdown","metadata":{"id":"1mQAauNGJy-v"},"source":["Correlation doesn't imply **causation** on its own. E.g., sepal length and petal length are extremely highly correlated, but this doesn't imply that sepal length causes petal length or vice versa. (Thousands of spurious correlations are provided [here](https://www.tylervigen.com/spurious-correlations) for your amusement.)"]},{"cell_type":"markdown","metadata":{"id":"Wh7hRf-NJy-w"},"source":["In brief, three criteria are required for inferring causal relationships:\n","\n","1. **Covariation**: Two variables vary together (this criterion is satisfied by sepal and petal length)\n","2. **Temporal precedence**: The affected variable must vary *after* the causal variable is varied.\n","3. **Elimination of extraneous variables**: We must be sure no third variable is causing the variation. This can be tricky for data we obtained through observation alone, but easier when we can control the causal variable, e.g., with (ideally double-blind) randomized control trials."]},{"cell_type":"markdown","metadata":{"id":"PrZMto2nJy-w"},"source":["Some examples of where we could infer causality from correlation in ML:\n","* Additional neurons --> higher accuracy\n","* Additional servers or RAM --> shorter inference time\n","* Removal of pronouns --> less demographic bias in model"]},{"cell_type":"markdown","metadata":{"id":"uuSMxo6uJy-w"},"source":["### Correcting for Multiple Comparisons"]},{"cell_type":"markdown","metadata":{"id":"eVGXGnYTJy-w"},"source":["A major issue with frequentist statistics is the issue of multiple comparisons:\n","\n","* If you perform 20 statistical tests where there is no real effect (i.e., the null hypothesis is true), then we would expect one of them to come up significant by chance alone (i.e., a *false positive* or *Type I error*).\n","* If you perform a hundred tests in such a circumstance, then you should expect five false positives."]},{"cell_type":"markdown","metadata":{"id":"HjCTRjYWJy-w"},"source":["The most straightforward, and indeed the most widely-used, solution is the **Bonferroni correction** (named after the 20th c. Italian mathematician Carlo Emilio Bonferroni). Assuming, we'd like an overall $\\alpha = .05$:\n","\n","* If we're planning on conducting ten tests ($m=10$), the significance threshold for each individual test is $\\frac{\\alpha}{m} = \\frac{.05}{10} = .005$\n","* With 20 tests, it's $\\frac{\\alpha}{m} = \\frac{.05}{20} = .0025$\n","* With 100 tests, it's $\\frac{\\alpha}{m} = \\frac{.05}{100} = .0005$"]},{"cell_type":"markdown","metadata":{"id":"yzexynOdJy-w"},"source":["(Other, less straightforward, approaches for adjusting $\\alpha$ for multiple comparisons exist. They're beyond our scope, but the major ones are listed under the *General methods of alpha adjustment for multiple comparisons* heading [here](https://en.wikipedia.org/wiki/Multiple_comparisons_problem#See_also).)"]},{"cell_type":"markdown","metadata":{"id":"eCf6c3OSJy-w"},"source":["##  2: Regression"]},{"cell_type":"markdown","metadata":{"id":"RxcUrV5EJy-w"},"source":["### Linear Least Squares for Fitting a Line to Points on a Cartesian Plane"]},{"cell_type":"code","metadata":{"id":"xt2m2UdYJy-x"},"source":["_ = sns.scatterplot(x=x, y=y)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"_JFRDiSHJy-x"},"source":["Consider fitting a line to points on a **Cartesian plane** (2-D surface, with $y$-axis perpendicular to horizontal $x$-axis). To fit such a line, the only parameters we require are a $y$-intercept (say, $\\beta_0$) and a slope (say, $\\beta_1$):\n","\n","$$ y = \\beta_0 + \\beta_1 x $$\n","\n","This corresponds to the case where we have a single feature (a single predictor variable, $x$) in a regression model:\n","\n","$$ y = \\beta_0 + \\beta_1 x + \\epsilon $$\n","\n","The $\\epsilon$ term denotes **error**. For a given instance $i$, $\\epsilon_i$ is a measure of the difference between the true $y_i$ and the model's estimate, $\\hat{y}_i$. If the model predicts $y_i$ perfectly, then $\\epsilon_i = 0$.\n","\n","Our objective is to find the parameters $\\beta_0$ and $\\beta_1$ that minimize $\\epsilon$ across all the available data points.\n","\n","(Note that sepal length may not be an ideal example of a predictor variable, but these iris data are conveniently available at this stage of the notebook.)"]},{"cell_type":"markdown","metadata":{"id":"9O_ZEib_Jy-x"},"source":["In the case of a model with a single predictor $x$, there is a fairly straightforward **linear least squares** formula we can use to estimate $\\beta_1$:\n","$$ \\hat{\\beta}_1 = \\frac{\\text{cov}(x,y)}{\\sigma^2_x} $$"]},{"cell_type":"markdown","metadata":{"id":"USupBWpjJy-x"},"source":["(We'll dig further into the \"least squares\" concept in the next section, for now we can think of it as minimizing the squared error $(\\hat{y}_i - y_i)^2$, which we isolate from $\\text{cov}(x,y)$ via division by $\\sigma^2_x$)"]},{"cell_type":"code","metadata":{"id":"wE0qSO0LJy-x"},"source":["cov"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lkpIm5ZeJy-y"},"source":["beta1 = cov/np.var(x)\n","beta1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HiC4PvwPJy-y"},"source":["With $\\hat{\\beta}_1$ in hand, we can then rearrange the line equation ($y = \\beta_0 + \\beta_1 x$) to estimate $\\beta_0$:\n","$$ \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x} $$"]},{"cell_type":"code","metadata":{"id":"DMo0_f7DJy-y"},"source":["beta0 = ybar - beta1*xbar\n","beta0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3j9GFB4OJy-y"},"source":["xline = np.linspace(4, 8, 1000)\n","yline = beta0 + beta1*xline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1x-YTnpHJy-y"},"source":["sns.scatterplot(x=x, y=y)\n","_ = plt.plot(xline, yline, color='orange')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8XzKSflaJy-y"},"source":["In regression model terms, if we were provided with a sepal length $x_i$ we could now use the parameter estimates $\\hat{\\beta}_0$ and $\\hat{\\beta}_1$ to predict the petal length of an iris:\n","$$ \\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i $$"]},{"cell_type":"markdown","metadata":{"id":"lFOcI346Jy-z"},"source":["For instance, our model predicts that an iris with a 5.5cm-long sepal would have 3.1cm-long petal:"]},{"cell_type":"code","metadata":{"id":"hnFtPTnHJy-z"},"source":["x_i = 5.5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5eUHYRQnJy-z"},"source":["y_i = beta0 + beta1*x_i\n","y_i"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7EdyXhVPJy-z"},"source":["sns.scatterplot(x=x, y=y)\n","plt.plot(xline, yline, color='orange')\n","_ = plt.scatter(x_i, y_i, marker='o', color='purple')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HrSoRDT1Jy-z"},"source":["As a second example, using the same simulated \"Alzheimer's drug\" data as the [*Regression in PyTorch* notebook](https://github.com/jonkrohn/ML-foundations/blob/master/notebooks/regression-in-pytorch.ipynb) and several others in the *ML Foundations* series:"]},{"cell_type":"code","metadata":{"id":"t6H35qYXJy-z"},"source":["x = np.array([0, 1, 2, 3, 4, 5, 6, 7.])\n","y = np.array([1.86, 1.31, .62, .33, .09, -.67, -1.23, -1.37])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uhPksXevJy-z"},"source":["sns.scatterplot(x=x, y=y)\n","plt.title(\"Clinical Trial\")\n","plt.xlabel(\"Drug dosage (mL)\")\n","_ = plt.ylabel(\"Forgetfulness\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wlJ8-81iJy-z"},"source":["cov_mat = np.cov(x, y)\n","cov_mat"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["Recalling from above that:\n","$$ \\hat{\\beta}_1 = \\frac{\\text{cov}(x,y)}{\\sigma^2_x} $$"],"metadata":{"id":"iR3Bmk6uRzKa"}},{"cell_type":"code","metadata":{"id":"cVIviYSUJy-0"},"source":["beta1 = cov_mat[0,1]/cov_mat[0,0]\n","beta1"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["...and that:\n","$$ \\hat{\\beta}_0 = \\bar{y} - \\hat{\\beta}_1 \\bar{x} $$"],"metadata":{"id":"nNjoXFRLR_N3"}},{"cell_type":"code","metadata":{"id":"YrQUCrxkJy-0"},"source":["beta0 = y.mean() - beta1*x.mean()\n","beta0"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["...and, of course, our regression formula:\n","$$ \\hat{y}_i = \\hat{\\beta}_0 + \\hat{\\beta}_1 x_i $$"],"metadata":{"id":"hiwYNJzCSOca"}},{"cell_type":"code","metadata":{"id":"h2WTwF-mJy-0"},"source":["xline = np.linspace(0, 7, 1000)\n","yline = beta0 + beta1*xline"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XMx4vEgxJy-0"},"source":["By administering 4.5mL of the drug, our model predicts a forgetfulness score of -0.35:"]},{"cell_type":"code","metadata":{"id":"_OoPnTyaJy-1"},"source":["x_i = 4.5"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DvFBImXcJy-1"},"source":["y_i = beta0 + beta1*x_i\n","y_i"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8_e_U2jiJy-1"},"source":["sns.scatterplot(x=x, y=y)\n","plt.title(\"Clinical Trial\")\n","plt.xlabel(\"Drug dosage (mL)\")\n","plt.ylabel(\"Forgetfulness\")\n","plt.plot(xline, yline, color='orange')\n","_ = plt.scatter(x_i, y_i, marker='o', color='purple')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UoSetfajJy-2"},"source":["**Exercise**: With data from female Adélie penguins, create a linear least squares model that predicts body mass with flipper length. Predict the mass of a female Adélie penguin that has a flipper length of 197mm."]},{"cell_type":"code","metadata":{"id":"bAHMnL66lFqC"},"source":["adelie.head()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ez3YcuanhuHI"},"source":["x = adelie[adelie.sex == 'Female']['flipper_length_mm'].to_numpy()\n","y = adelie[adelie.sex == 'Female']['body_mass_g'].to_numpy()/1000"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U7piwPvujy_S"},"source":["_ = sns.scatterplot(x=x, y=y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gqUW4gVailR5"},"source":["cov_mat = np.cov(x, y)\n","cov_mat"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UyX0jsOVjD6V"},"source":["beta1 = cov_mat[0,1]/cov_mat[0,0]\n","beta1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"0JO7HmUgjEYd"},"source":["beta0 = y.mean() - beta1*x.mean()\n","beta0"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o2ZfBlXUnFrL"},"source":["x_i = 197"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iOEfmJ7Um9Zz"},"source":["y_i = beta0 + beta1*x_i\n","y_i"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QuGxsnomjHau"},"source":["xline = np.linspace(170, 205, 1000)\n","yline = beta0 + beta1*xline"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6_muUi5rkCoM"},"source":["sns.scatterplot(x=x, y=y)\n","plt.title(\"Female Adélie Penguins\")\n","plt.xlabel(\"Flipper Length (mm)\")\n","plt.ylabel(\"Body Mass (kg)\")\n","plt.plot(xline, yline, color='orange')\n","_ = plt.scatter(x_i, y_i, marker='o', color='purple')"],"execution_count":null,"outputs":[]}]}