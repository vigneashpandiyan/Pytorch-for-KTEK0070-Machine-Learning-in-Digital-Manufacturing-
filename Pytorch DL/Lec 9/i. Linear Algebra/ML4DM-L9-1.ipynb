{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"}},"cells":[{"cell_type":"markdown","metadata":{"id":"view-in-github"},"source":["<a href=\"https://vigneashpandiyan.github.io/publications/Codes/\" target=\"_blank\" rel=\"noopener noreferrer\">\n","  <img src=\"https://vigneashpandiyan.github.io/images/Link.png\"\n","       style=\"max-width: 800px; width: 100%; height: auto;\">\n","</a>"]},{"cell_type":"markdown","metadata":{"id":"aTOLgsbN69-P"},"source":["# Intro to Linear Algebra"]},{"cell_type":"markdown","metadata":{"id":"yqUB9FTRAxd-"},"source":["Linear algebra lies at the heart of most machine learning approaches and is especially predominant in deep learning, the branch of ML at the forefront of today’s artificial intelligence advances. Through the measured exposition of theory paired with interactive examples, you’ll develop an understanding of how linear algebra is used to solve for unknown values in high-dimensional spaces, thereby enabling machines to recognize patterns and make predictions.\n"]},{"cell_type":"markdown","metadata":{"id":"d4tBvI88BheF"},"source":["Over the course of studying this topic, you'll:\n","\n","* Understand the fundamentals of linear algebra, a ubiquitous approach for solving for unknowns within high-dimensional spaces.\n","\n","* Develop a geometric intuition of what’s going on beneath the hood of machine learning algorithms, including those used for deep learning.\n","* Be able to more intimately grasp the details of machine learning papers as well as all of the other subjects that underlie ML, including calculus, statistics, and optimization algorithms."]},{"cell_type":"markdown","metadata":{"id":"2khww76J5w9n"},"source":["## 1: Data Structures for Algebra\n"]},{"cell_type":"markdown","metadata":{"id":"niG_MgK-iV6K"},"source":["### What Linear Algebra Is"]},{"cell_type":"code","metadata":{"id":"LApX90aliab_"},"source":["import numpy as np\n","import matplotlib.pyplot as plt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E4odh9Shic1S"},"source":["t = np.linspace(0, 40, 1000) # start, finish, n points"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"N-tYny12nIyO"},"source":["Distance travelled by robber: $d = 2.5t$"]},{"cell_type":"code","metadata":{"id":"e_zDOxgHiezz"},"source":["d_r = 2.5 * t"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"djVjXZy-nPaR"},"source":["Distance travelled by sheriff: $d = 3(t-5)$"]},{"cell_type":"code","metadata":{"id":"JtaNeYSCifrI"},"source":["d_s = 3 * (t-5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SaaIjJSEigic"},"source":["fig, ax = plt.subplots()\n","plt.title('A Bank Robber Caught')\n","plt.xlabel('time (in minutes)')\n","plt.ylabel('distance (in km)')\n","ax.set_xlim([0, 40])\n","ax.set_ylim([0, 100])\n","ax.plot(t, d_r, c='green')\n","ax.plot(t, d_s, c='brown')\n","plt.axvline(x=30, color='purple', linestyle='--')\n","_ = plt.axhline(y=75, color='purple', linestyle='--')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NgGMhK4B51oe"},"source":["### Scalars (Rank 0 Tensors) in Base Python"]},{"cell_type":"code","metadata":{"id":"ZXnTHDn_EW6b"},"source":["x = 25\n","x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VF8Jam76R4KJ"},"source":["type(x) # if we'd like more specificity (e.g., int16, uint8), we need NumPy or another numeric library"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZBzYlL0mRd-P"},"source":["y = 3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1i-hW0bcReyy"},"source":["py_sum = x + y\n","py_sum"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CpyUxB6XRk6y"},"source":["type(py_sum)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"V2UiLj-JR8Ij"},"source":["x_float = 25.0\n","float_sum = x_float + y\n","float_sum"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ikOwjp6ASCaf"},"source":["type(float_sum)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SgUvioyUz8T2"},"source":["### Scalars in PyTorch\n","\n","* PyTorch and TensorFlow are the two most popular *automatic differentiation* libraries\n","* PyTorch tensors are designed to be pythonic, i.e., to feel and behave like NumPy arrays.\n","* The advantage of PyTorch tensors relative to NumPy arrays is that they easily be used for operations on GPU"]},{"cell_type":"code","metadata":{"id":"A9Hhazt2zKeD"},"source":["import torch"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"a211IRW_0-iY"},"source":["x_pt = torch.tensor(25) # type specification optional, e.g.: dtype=torch.float16\n","x_pt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LvxzMa_HhUNB"},"source":["x_pt.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4CURG9Er6aZI"},"source":["### Vectors (Rank 1 Tensors) in NumPy"]},{"cell_type":"code","metadata":{"id":"T9ME4kBr4wg0"},"source":["x = np.array([25, 2, 5]) # type argument is optional, e.g.: dtype=np.float16\n","x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZuotxmlZL2wp"},"source":["len(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OlPYy6GOaIVy"},"source":["x.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sWbYGwObcgtK"},"source":["type(x)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ME_xuvD_oTPg"},"source":["x[0] # zero-indexed"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hXmBHZQ-nxFw"},"source":["type(x[0])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NiEofCzYZBrQ"},"source":["### Vector Transposition"]},{"cell_type":"code","metadata":{"id":"hxGFNDx6V95l"},"source":["# Transposing a regular 1-D array has no effect...\n","x_t = x.T\n","x_t"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_f8E9ExDWw4p"},"source":["x_t.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AEd8jB7YcgtT"},"source":["# ...but it does we use nested \"matrix-style\" brackets:\n","y = np.array([[25, 2, 5]])\n","y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UHQd92oRcgtV"},"source":["y.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SPi1JqGEXXUc"},"source":["# ...but can transpose a matrix with a dimension of length 1, which is mathematically equivalent:\n","y_t = y.T\n","y_t"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6rzUv762Yjis"},"source":["y_t.shape # this is a column vector as it has 3 rows and 1 column"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xVnQMLOrYtra"},"source":["# Column vector can be transposed back to original row vector:\n","y_t.T"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QIAA2NLRZIXC"},"source":["y_t.T.shape"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Voj26mSpZLuh"},"source":["### Zero Vectors\n","\n","Have no effect if added to another vector"]},{"cell_type":"code","metadata":{"id":"-46AbOdkZVn_"},"source":["z = np.zeros(3)\n","z"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"c6xyYiwSnSGC"},"source":["### Vectors in PyTorch"]},{"cell_type":"code","metadata":{"id":"s2TGDeqXnitZ"},"source":["x_pt = torch.tensor([25, 2, 5])\n","x_pt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8fU5qVTI6SLD"},"source":["### $L^2$ Norm"]},{"cell_type":"code","metadata":{"id":"lLc2FbGG6SLD"},"source":["x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AN43hsl86SLG"},"source":["(25**2 + 2**2 + 5**2)**(1/2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"D9CyWo-l6SLI"},"source":["np.linalg.norm(x)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TNEMRi926SLK"},"source":["So, if units in this 3-dimensional vector space are meters, then the vector $x$ has a length of 25.6m"]},{"cell_type":"markdown","metadata":{"id":"PwiRlMuC6SLK"},"source":["### $L^1$ Norm"]},{"cell_type":"code","metadata":{"id":"lcYKyc5H6SLL"},"source":["x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8jNb6nYl6SLM"},"source":["np.abs(25) + np.abs(2) + np.abs(5)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lQP73B916SLP"},"source":["### Squared $L^2$ Norm"]},{"cell_type":"code","metadata":{"id":"Qv1ouJ8r6SLP"},"source":["x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eG3WSB5R6SLT"},"source":["(25**2 + 2**2 + 5**2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bXwzSudS6SLV"},"source":["# we'll cover tensor multiplication more soon but to prove point quickly:\n","np.dot(x, x)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BHWxVPFC6SLX"},"source":["### Max Norm"]},{"cell_type":"code","metadata":{"id":"vO-zfvDG6SLX"},"source":["x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vXXLgbyW6SLZ"},"source":["np.max([np.abs(25), np.abs(2), np.abs(5)])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JzKlIpYZcgt9"},"source":["### Orthogonal Vectors"]},{"cell_type":"code","metadata":{"id":"4jHg9La-cgt9"},"source":["i = np.array([1, 0])\n","i"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3FyLhPK3cguA"},"source":["j = np.array([0, 1])\n","j"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7eQtKhaDcguC","scrolled":false},"source":["np.dot(i, j) # detail on the dot operation coming up..."],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"mK3AZH53o8Br"},"source":["### Matrices (Rank 2 Tensors) in NumPy"]},{"cell_type":"code","metadata":{"id":"stk57cmaESW1"},"source":["# Use array() with nested brackets:\n","X = np.array([[25, 2], [5, 26], [3, 7]])\n","X"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"IhDL4L8S6SLc"},"source":["X.shape"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"q3oyaAK36SLe"},"source":["X.size"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YN9CHzja6SLg"},"source":["# Select left column of matrix X (zero-indexed)\n","X[:,0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ih7nh4qC6SLi"},"source":["# Select middle row of matrix X:\n","X[1,:]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pg7numxP6SLl"},"source":["# Another slicing-by-index example:\n","X[0:2, 0:2]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HGEfZiBb6SLt"},"source":["### Matrices in PyTorch"]},{"cell_type":"code","metadata":{"id":"-bibT9ye6SLt"},"source":["X_pt = torch.tensor([[25, 2], [5, 26], [3, 7]])\n","X_pt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"TBPu1L7P6SLv"},"source":["X_pt.shape # pythonic relative to TensorFlow"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4mTj56M16SLw"},"source":["X_pt[1,:] # N.B.: Python is zero-indexed; written algebra is one-indexed"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"cMpfujF_6SLy"},"source":["### Higher-Rank Tensors\n","\n","As an example, rank 4 tensors are common for images, where each dimension corresponds to:\n","\n","1. Number of images in training batch, e.g., 32\n","2. Image height in pixels, e.g., 28 for [MNIST digits](http://yann.lecun.com/exdb/mnist/)\n","3. Image width in pixels, e.g., 28\n","4. Number of color channels, e.g., 3 for full-color images (RGB)"]},{"cell_type":"code","metadata":{"id":"KSZlICRR6SL1"},"source":["images_pt = torch.zeros([32, 28, 28, 3])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"6Dqj0vmh6SL2"},"source":["# images_pt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lmG3LEZK6SL4"},"source":["##2: Common Tensor Operations"]},{"cell_type":"markdown","metadata":{"id":"iSHGMCxd6SL4"},"source":["### Tensor Transposition"]},{"cell_type":"code","metadata":{"id":"1YN1narR6SL4"},"source":["X"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5hf3M_NL6SL5"},"source":["X.T"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vyBFN_4g6SL9"},"source":["X_pt.T"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Hp9P1jx76SL_"},"source":["### Basic Arithmetical Properties"]},{"cell_type":"markdown","metadata":{"id":"WxaImEUc6SMA"},"source":["Adding or multiplying with scalar applies operation to all elements and tensor shape is retained:"]},{"cell_type":"code","metadata":{"id":"yhXGETii6SMA"},"source":["X*2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KnPULtDO6SMC"},"source":["X+2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MkfC0Gsb6SMD"},"source":["X*2+2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"04bIDpGj6SMH"},"source":["X_pt*2+2 # Python operators are overloaded; could alternatively use torch.mul() or torch.add()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"2oRBSmRL6SMI"},"source":["torch.add(torch.mul(X_pt, 2), 2)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wt8Ls4076SMK"},"source":["If two tensors have the same size, operations are often by default applied element-wise. This is **not matrix multiplication**, which we'll cover later, but is rather called the **Hadamard product** or simply the **element-wise product**.\n","\n","The mathematical notation is $A \\odot X$"]},{"cell_type":"code","metadata":{"id":"KUMyU1t46SMK"},"source":["X"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UNIbp0P36SML"},"source":["A = X+2\n","A"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HE9xPWPdcgu4"},"source":["A + X"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xKyCwGia6SMP"},"source":["A * X"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"B5jXGIBp6SMT"},"source":["A_pt = X_pt + 2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"A7k6yxu36SMU"},"source":["A_pt + X_pt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"r8vOul0m6SMW"},"source":["A_pt * X_pt"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FE5f-FEq6SMY"},"source":["### Reduction"]},{"cell_type":"markdown","metadata":{"id":"WPJ9FVQF6SMY"},"source":["Calculating the sum across all elements of a tensor is a common operation. For example:\n","\n","* For vector ***x*** of length *n*, we calculate $\\sum_{i=1}^{n} x_i$\n","* For matrix ***X*** with *m* by *n* dimensions, we calculate $\\sum_{i=1}^{m} \\sum_{j=1}^{n} X_{i,j}$"]},{"cell_type":"code","metadata":{"id":"rXi2stvz6SMZ"},"source":["X"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"W9FKaJbf6SMZ"},"source":["X.sum()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3y9aw7t66SMc"},"source":["torch.sum(X_pt)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"awjH9bOz6SMc"},"source":["# Can also be done along one specific axis alone, e.g.:\n","X.sum(axis=0) # summing over all rows (i.e., along columns)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n2SASjsn6SMd"},"source":["X.sum(axis=1) # summing over all columns (i.e., along rows)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uVnSxvSJ6SMh"},"source":["torch.sum(X_pt, 0)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gdAe8S4A6SMj"},"source":["Many other operations can be applied with reduction along all or a selection of axes, e.g.:\n","\n","* maximum\n","* minimum\n","* mean\n","* product\n","\n","They're fairly straightforward and used less often than summation, so you're welcome to look them up in library docs if you ever need them."]},{"cell_type":"markdown","metadata":{"id":"r2eW8S_46SMj"},"source":["### The Dot Product"]},{"cell_type":"markdown","metadata":{"id":"LImETgD76SMj"},"source":["If we have two vectors (say, ***x*** and ***y***) with the same length *n*, we can calculate the dot product between them. This is annotated several different ways, including the following:\n","\n","* $x \\cdot y$\n","* $x^Ty$\n","* $\\langle x,y \\rangle$\n","\n","Regardless which notation you use (I prefer the first), the calculation is the same; we calculate products in an element-wise fashion and then sum reductively across the products to a scalar value. That is, $x \\cdot y = \\sum_{i=1}^{n} x_i y_i$\n","\n","The dot product is ubiquitous in deep learning: It is performed at every artificial neuron in a deep neural network, which may be made up of millions (or orders of magnitude more) of these neurons."]},{"cell_type":"code","metadata":{"id":"HveIE3IDcgvP"},"source":["x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3ZjkZcvVcgvQ"},"source":["y = np.array([0, 1, 2])\n","y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Xu8z0QB0cgvR"},"source":["25*0 + 2*1 + 5*2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ThehRrr8cgvS"},"source":["np.dot(x, y)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J5Zdua4xcgvT"},"source":["x_pt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"b3vEdroXcgvU"},"source":["y_pt = torch.tensor([0, 1, 2])\n","y_pt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"F741E5imcgvV"},"source":["np.dot(x_pt, y_pt)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-W5loHc8cgvX"},"source":["torch.dot(torch.tensor([25, 2, 5.]), torch.tensor([0, 1, 2.]))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bYDhomCP6SMj"},"source":["##3: Matrix Properties"]},{"cell_type":"markdown","metadata":{"id":"-HGU_an66SMk"},"source":["### Frobenius Norm"]},{"cell_type":"code","metadata":{"id":"pNQHvAqN6SMk"},"source":["X = np.array([[1, 2], [3, 4]])\n","X"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"T-q-Tzn26SMm"},"source":["(1**2 + 2**2 + 3**2 + 4**2)**(1/2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"YVG8qiFw6SMn"},"source":["np.linalg.norm(X) # same function as for vector L2 norm"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"FPnBflKVxyik"},"source":["X_pt = torch.tensor([[1, 2], [3, 4.]]) # torch.norm() supports floats only"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"NCdTShVyx8z0"},"source":["torch.norm(X_pt)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"OLN-MMIe6SMo"},"source":["### Matrix Multiplication (with a Vector)"]},{"cell_type":"code","metadata":{"id":"XJw0j8cr6SMo"},"source":["A = np.array([[3, 4], [5, 6], [7, 8]])\n","A"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"zZQ1Aupc6SMq"},"source":["b = np.array([1, 2])\n","b"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZbeVtNyW6SMq"},"source":["np.dot(A, b) # even though technically dot products are between vectors only"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"srVI55X96SMu"},"source":["A_pt = torch.tensor([[3, 4], [5, 6], [7, 8]])\n","A_pt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"5SDn71Xc6SMv"},"source":["b_pt = torch.tensor([1, 2])\n","b_pt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OIeoJlsh6SMx"},"source":["torch.matmul(A_pt, b_pt) # like np.dot(), automatically infers dims in order to perform dot product, matvec, or matrix multiplication"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"21ySqay36SM5"},"source":["### Matrix Multiplication (with Two Matrices)"]},{"cell_type":"code","metadata":{"id":"0YRG1Ig2cgvo"},"source":["A"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DyOEZk_c6SM5"},"source":["B = np.array([[1, 9], [2, 0]])\n","B"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SfKuNxH-6SM6"},"source":["np.dot(A, B)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"WcnQMF0s6SNB"},"source":["Note that matrix multiplication is not \"commutative\" (i.e., $AB \\neq BA$) so uncommenting the following line will throw a size mismatch error:"]},{"cell_type":"code","metadata":{"id":"_mwBGOXO6SNB"},"source":["# np.dot(B, A)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JrrvPoNE6SM9"},"source":["B_pt = torch.from_numpy(B) # much cleaner than TF conversion\n","B_pt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z6PfwCvX6SM-"},"source":["# another neat way to create the same tensor with transposition:\n","B_pt = torch.tensor([[1, 2], [9, 0]]).T\n","B_pt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"16ZNRaVe6SM_"},"source":["torch.matmul(A_pt, B_pt) # no need to change functions, unlike in TF"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"L2H9F-DQ6SMz"},"source":["### Symmetric Matrices"]},{"cell_type":"code","metadata":{"id":"5YsPoWo76SMz"},"source":["X_sym = np.array([[0, 1, 2], [1, 7, 8], [2, 8, 9]])\n","X_sym"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Skg1wSQVcgv2"},"source":["X_sym.T"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Jv40-i9H6SM1"},"source":["X_sym.T == X_sym"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Mq_c3ftZ6SM2"},"source":["### Identity Matrices"]},{"cell_type":"code","metadata":{"id":"KVSNbH-Z6SM2"},"source":["I = torch.tensor([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n","I"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wcoPDhvR6SM3"},"source":["x_pt = torch.tensor([25, 2, 5])\n","x_pt"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tuA4RsMv6SM4"},"source":["torch.matmul(I, x_pt)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"3S_6Yfdkcgv7"},"source":["### Answers to Matrix Multiplication Qs"]},{"cell_type":"code","metadata":{"id":"pINsKNxH6SNC"},"source":["M_q = torch.tensor([[0, 1, 2], [3, 4, 5], [6, 7, 8]])\n","M_q"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gfjWd8OO6SNE"},"source":["V_q = torch.tensor([[-1, 1, -2], [0, 1, 2]]).T\n","V_q"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"boSkaV2M6SNF"},"source":["torch.matmul(M_q, V_q)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"slSNKUcN6SNG"},"source":["### Matrix Inversion"]},{"cell_type":"code","metadata":{"id":"EW0i5ZRk6SNG"},"source":["X = np.array([[4, 2], [-5, -3]])\n","X"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hTYpxaWR6SNI"},"source":["Xinv = np.linalg.inv(X)\n","Xinv"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"XFDBBdYOc-7E"},"source":["As a quick aside, let's prove that $X^{-1}X = I_n$ :"]},{"cell_type":"code","metadata":{"id":"tyPhf-uZcvdB"},"source":["np.dot(Xinv, X)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"SVwvbBvclul1"},"source":["...and now back to solving for the unknowns in $w$:"]},{"cell_type":"code","metadata":{"id":"Q5sQqFaz6SNK"},"source":["y = np.array([4, -7])\n","y"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PK7m6F1I6SNL"},"source":["w = np.dot(Xinv, y)\n","w"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"fyBOHgdccgwD"},"source":["Show that $y = Xw$:"]},{"cell_type":"code","metadata":{"id":"SVBojjwacgwD"},"source":["np.dot(X, w)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rKELsi0PZCr0"},"source":["**Geometric Visualization**\n","\n","Suppose the two equations in the system are:\n","$$ 4b + 2c = 4 $$\n","$$ -5b - 3c = -7 $$\n","\n","Both equations can be rearranged to isolate a variable, say $c$. Starting with the first equation:\n","$$ 4b + 2c = 4 $$\n","$$ 2b + c = 2 $$\n","$$ c = 2 - 2b $$\n","\n","Then for the second equation:\n","$$ -5b - 3c = -7 $$\n","$$ -3c = -7 + 5b $$\n","$$ c = \\frac{-7 + 5b}{-3} = \\frac{7 - 5b}{3} $$"]},{"cell_type":"code","metadata":{"id":"ZASIBqroap7k"},"source":["b = np.linspace(-10, 10, 1000) # start, finish, n points"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9XsUdVsmaqTr"},"source":["c1 = 2 - 2*b"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8Mwn1tAca1Cl"},"source":["c2 = (7-5*b)/3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"X5ozP9jZauQa"},"source":["fig, ax = plt.subplots()\n","plt.xlabel('b', c='darkorange')\n","plt.ylabel('c', c='brown')\n","\n","plt.axvline(x=0, color='lightgray')\n","plt.axhline(y=0, color='lightgray')\n","\n","ax.set_xlim([-2, 3])\n","ax.set_ylim([-1, 5])\n","ax.plot(b, c1, c='purple')\n","ax.plot(b, c2, c='purple')\n","plt.axvline(x=-1, color='green', linestyle='--')\n","_ = plt.axhline(y=4, color='green', linestyle='--')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"14uhePTna7ZV"},"source":["In PyTorch :"]},{"cell_type":"code","metadata":{"id":"7qo-SAvaansp"},"source":["torch.inverse(torch.tensor([[4, 2], [-5, -3.]])) # float type"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hcP2S4AMAHT_"},"source":["**Exercises**:\n","\n","1. As done with NumPy above, use PyTorch to calculate $w$ from $X$ and $y$. Subsequently, confirm that $y = Xw$.\n","2. Repeat again, now using TensorFlow."]},{"cell_type":"markdown","metadata":{"id":"N8ZxpgcN6SNO"},"source":["### Matrix Inversion Where No Solution"]},{"cell_type":"code","metadata":{"id":"RYORHY4E6SNO"},"source":["X = np.array([[-4, 1], [-8, 2]])\n","X"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o7GcISdI6SNP"},"source":["# Uncommenting the following line results in a \"singular matrix\" error\n","# Xinv = np.linalg.inv(X)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"BXomEBoyDQyO"},"source":["### Orthogonal Matrices\n","\n","An orthogonal matrix is a square matrix whose transpose is also its inverse.\n","\n","To demonstrate the matrix $I_3$ has mutually orthogonal columns, we show that the dot product of any pair of columns is zero:"]},{"cell_type":"code","metadata":{"id":"-Un4Aq805HM1"},"source":["I = np.array([[1, 0, 0], [0, 1, 0], [0, 0, 1]])\n","I"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"CUhMi2_IEyaI"},"source":["column_1 = I[:,0]\n","column_1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xsOtn-siE63K"},"source":["column_2 = I[:,1]\n","column_2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Pv3b51ExE9ZX"},"source":["column_3 = I[:,2]\n","column_3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VhnRlc8eElYc"},"source":["np.dot(column_1, column_2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mVq4m-LfFGjQ"},"source":["np.dot(column_1, column_3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"nJj-aihzFILx"},"source":["np.dot(column_2, column_3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FIzpkdJmFYdY"},"source":["We can use the `np.linalg.norm()` method from earlier in the notebook to demonstrate that each column of $I_3$ has unit norm:"]},{"cell_type":"code","metadata":{"id":"xKbbeqegFJSc"},"source":["np.linalg.norm(column_1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ycv0L8mpGKRC"},"source":["np.linalg.norm(column_2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LsEuRezYGLgY"},"source":["np.linalg.norm(column_3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NMrHdyrsGQ1X"},"source":["Since the matrix $I_3$ has mutually orthogonal columns and each column has unit norm, the column vectors of $I_3$ are *orthonormal*. Since $I_3^T = I_3$, this means that the *rows* of $I_3$ must also be orthonormal.\n","\n","Since the columns and rows of $I_3$ are orthonormal, $I_3$ is an *orthogonal matrix*."]},{"cell_type":"markdown","metadata":{"id":"K7EykcPdIZhE"},"source":["Now using matrix *K* instead of $I_3$. This time using PyTorch."]},{"cell_type":"code","metadata":{"id":"MXVyu8tSGMlZ"},"source":["K = torch.tensor([[2/3, 1/3, 2/3], [-2/3, 2/3, 1/3], [1/3, 2/3, -2/3]])\n","K"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"xqkmSYNfJmTs"},"source":["Kcol_1 = K[:,0]\n","Kcol_1"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ncoc3IxNJ0v2"},"source":["Kcol_2 = K[:,1]\n","Kcol_2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ISEKQ0AqJ2wf"},"source":["Kcol_3 = K[:,2]\n","Kcol_3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"jvdXuMzbJ44d"},"source":["torch.dot(Kcol_1, Kcol_2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BUR60eBuKEbU"},"source":["torch.dot(Kcol_1, Kcol_3)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Rg7QvQwuKGJZ"},"source":["torch.dot(Kcol_2, Kcol_3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wNdV36-QKNK-"},"source":["We've now determined that the columns of $K$ are orthogonal."]},{"cell_type":"code","metadata":{"id":"DDgyhj1AKHdS"},"source":["torch.norm(Kcol_1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KvQZy_E2KorW"},"source":["torch.norm(Kcol_2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7H7dx8a7Kp9f"},"source":["torch.norm(Kcol_3)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VPPtOk5OKtJ_"},"source":["We've now determined that, in addition to being orthogonal, the columns of $K$ have unit norm, therefore they are orthonormal.\n","\n","To ensure that $K$ is an orthogonal matrix, we would need to show that not only does it have orthonormal columns but it has orthonormal rows are as well. Since $K^T \\neq K$, we can't prove this quite as straightforwardly as we did with $I_3$.\n","\n","One approach would be to repeat the steps we used to determine that $K$ has orthogonal columns with all of the matrix's rows (please feel free to do so). Alternatively, we can use an orthogonal matrix-specific equation, $A^TA = I$, to demonstrate that $K$ is orthogonal in a single line of code:"]},{"cell_type":"code","metadata":{"id":"W8Ao0KhVNHqF"},"source":["torch.matmul(K.T, K)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"scM6FCpjNgYA"},"source":["Notwithstanding rounding errors that we can safely ignore, this confirms that $K^TK = I$ and therefore $K$ is an orthogonal matrix."]}]}