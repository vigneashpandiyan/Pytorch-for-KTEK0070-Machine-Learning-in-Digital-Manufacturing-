{"cells":[{"cell_type":"markdown","source":["<a href=\"https://vigneashpandiyan.github.io/publications/Codes/\" target=\"_blank\" rel=\"noopener noreferrer\">\n","  <img src=\"https://vigneashpandiyan.github.io/images/Link.png\"\n","       style=\"max-width: 800px; width: 100%; height: auto;\">\n","</a>"],"metadata":{"id":"RjpbI9jMsvTi"}},{"cell_type":"markdown","metadata":{"id":"y3yzFoWz8I6h"},"source":[" Auto-differentiation\n","================\n","\n","Auto-differentiation is a feature in libraries like PyTorch and TensorFlow that automatically calculates derivatives, or gradients, for you. The library keeps track of every step and calculates the gradients automatically, no matter how complicated your calculations are. This makes it much easier and faster to train machine learning models.\n","\n","If you flag a torch Tensor with the attribute `x.requires_grad=True`, then pytorch will automatically keep track the computational history of all tensors that are derived from `x`.  This allows pytorch to figure out derivatives of any scalar result with regard to changes in the components of x.\n","\n","The function `torch.autograd.grad(output_scalar, [list of input_tensors])` computes `d(output_scalar)/d(input_tensor)` for each input tensor component in the list.  For it to work, the input tensors and output must be part of the same `requires_grad=True` computation.\n","\n","In the example here, `x` is explicitly marked `requires_grad=True`, so `y.sum()`, which is derived from `x`, automatically comes along with the computation history, and can be differentiated."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mRV_o0mL8I6k"},"outputs":[],"source":["import torch\n","from matplotlib import pyplot as plt\n","\n","x = torch.linspace(0, 5, 100,\n","          requires_grad=True)\n","y = (x**2).cos()\n","s = y.sum()\n","[dydx] = torch.autograd.grad(s, [x])\n","\n","plt.plot(x.detach(), y.detach(), label='y')\n","plt.plot(x.detach(), dydx, label='dy/dx')\n","plt.legend()\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"e-l5lg1C8I6l"},"source":["(Note that in the example above, because the components of the vector space are independent of each other, we happen to have `dy[j] / dx[i] == 0` when `j != i`, so that `d(y.sum())/dx[i] = dy[i]/dx[i]`.  That means computing a single gradient vector of the sum `s` is equivalent to computing elementwise derivatives `dy/dx`.)\n","\n","**Detaching tensors from the computation history.**\n","\n","Every tensor that depends on `x` will be `requires_grad=True` and connected to the complete computation history. But if you were to convert a tensor to a regular python number, pytorch would not be able to see the calculations and would not be able to compute gradients on it.\n","\n","To avoid programming mistakes where some computation invisibly goes through a non-pytorch number that cannot be tracked, pytorch disables requires-grad tensors from being converted to untrackable numbers.  You need to explicitly call `x.detach()` or `y.detach()` first, to explicitly say that you want an untracked reference, before plotting the data or using it as non-pytorch numbers."]},{"cell_type":"markdown","metadata":{"id":"p0WErLQX8I6m"},"source":["### Exercise\n","\n","Plot the polynomial y=x<sup>3</sup>-6x<sup>2</sup>+8x and its derivative"]},{"cell_type":"code","source":["import torch\n","import matplotlib.pyplot as plt\n","import numpy as np\n","\n","def plot_pytorch_derivative():\n","    # 1. SETUP DATA\n","    # Create x values from -2 to 6\n","    x_np = np.linspace(-2, 6, 100)\n","\n","    # Convert to PyTorch Tensor and enable gradient tracking\n","    x = torch.tensor(x_np, requires_grad=True, dtype=torch.float32)\n","\n","    # 2. DEFINE FUNCTION\n","    # y = x^3 - 6x^2 + 8x\n","    y = x**3 - 6*x**2 + 8*x\n","\n","    # 3. COMPUTE DERIVATIVE\n","    # We call backward() to compute gradients.\n","    # Since y is a vector, we pass torch.ones_like(x) to get element-wise derivatives.\n","    y.backward(torch.ones_like(x))\n","\n","    # Retrieve the calculated derivative from x.grad\n","    dy = x.grad\n","\n","    # 4. PLOT\n","    plt.figure(figsize=(10, 6))\n","\n","    # We must use .detach().numpy() to convert PyTorch tensors back to standard numPy arrays for plotting\n","    plt.plot(x.detach().numpy(), y.detach().numpy(), label='$y = x^3 - 6x^2 + 8x$')\n","    plt.plot(x.detach().numpy(), dy.detach().numpy(), label=\"Derivative ($y'$)\", linestyle='--')\n","\n","    plt.axhline(0, color='black', alpha=0.3)\n","    plt.axvline(0, color='black', alpha=0.3)\n","    plt.legend()\n","    plt.grid(True, alpha=0.3)\n","    plt.title(\"Function vs Derivative (Calculated by PyTorch)\")\n","    plt.xlabel(\"x\")\n","    plt.ylabel(\"y\")\n","\n","    plt.show()\n","\n","if __name__ == \"__main__\":\n","    plot_pytorch_derivative()"],"metadata":{"id":"H4pC3Dy3TkqM"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"APvcsl9X8I6u"},"source":["More tricks\n","-----------\n","\n","**Gradients over intermediate values.** Normally gradients with respect to intermediate values are not stored in `.grad` - just original input variables - but you can ask for intermediate gradients to be stored using `v.retain_grad()`.\n","\n","**Second derivatives.** If you want higher-order derivatives, then you want pytorch to build the computation graph when it is computing the gradient itself, so this graph can be differentiated again.  To do this, use the `create_graph=True` option on the `grad` or `backward` methods.\n","\n","**Gradients of more than one objective.** Usually you only need to call `y.backward()` once per computation tree, and pytorch will not let you call it again. To save memory, pytorch will have deallocated the computation graph after you have computed a single gradient.  But if you need more than one gradient (e.g., if you have different objectives that you want to apply to different subsets of parameters, as with happens with GANs sometimes), you can use `retain_graph=True`.\n"]},{"cell_type":"code","source":["import torch\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","def visualize_advanced_autograd():\n","    # 1. SETUP DATA\n","    # Create a vector of x values\n","    x_np = np.linspace(-6, 6, 200)\n","    x = torch.tensor(x_np, dtype=torch.float32, requires_grad=True)\n","\n","    # 2. DEFINE FUNCTION\n","    # y = sin(x) * (x^2 / 4)\n","    # We use this function because it has interesting curves\n","    y = torch.sin(x) * (x**2) / 4\n","\n","    # 3. FIRST DERIVATIVE (The Trick: create_graph=True)\n","    # We sum() y because autograd expects a scalar, but mathematically\n","    # this computes element-wise derivatives for our independent x values.\n","    # create_graph=True is the KEY: It lets us differentiate this result again.\n","    grads_1 = torch.autograd.grad(y.sum(), x, create_graph=True)[0]\n","\n","    # 4. SECOND DERIVATIVE\n","    # Now we differentiate the gradients themselves!\n","    grads_2 = torch.autograd.grad(grads_1.sum(), x)[0]\n","\n","    # 5. VISUALIZATION\n","    plt.figure(figsize=(12, 7))\n","\n","    # Detach tensors to convert to numpy for plotting\n","    x_val = x.detach().numpy()\n","    y_val = y.detach().numpy()\n","    dy_val = grads_1.detach().numpy()\n","    ddy_val = grads_2.detach().numpy()\n","\n","    # Plot all three curves\n","    plt.plot(x_val, y_val, label=r'Function $f(x)$', linewidth=3, color='blue')\n","    plt.plot(x_val, dy_val, label=r\"1st Derivative $f'(x)$\", linewidth=2, linestyle='--', color='orange')\n","    plt.plot(x_val, ddy_val, label=r\"2nd Derivative $f''(x)$\", linewidth=2, linestyle=':', color='green')\n","\n","    plt.title(\"Advanced Autograd: Visualizing Higher-Order Derivatives\\n(Using create_graph=True)\", fontsize=14)\n","    plt.xlabel(\"Input (x)\")\n","    plt.ylabel(\"Output (y)\")\n","    plt.axhline(0, color='black', alpha=0.3)\n","    plt.legend()\n","    plt.grid(True, alpha=0.3)\n","\n","    # Add a note about the trick\n","    plt.text(-5.5, 4,\n","             \"TRICK USED:\\ncreate_graph=True\\n\\nAllows differentiating\\nthe gradients themselves!\",\n","             fontsize=10, bbox=dict(facecolor='white', alpha=0.9))\n","\n","    plt.show()\n","\n","if __name__ == \"__main__\":\n","    visualize_advanced_autograd()"],"metadata":{"id":"GC8bIhj7UdEf"},"execution_count":null,"outputs":[]}],"metadata":{"accelerator":"GPU","kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.9"},"colab":{"provenance":[]}},"nbformat":4,"nbformat_minor":0}